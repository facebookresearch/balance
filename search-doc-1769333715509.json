[{"title":"Bringing \"balance\" to your data","type":0,"sectionRef":"#","url":"/blog/2023/01/09/bringing-balance-to-your-data/","content":"","keywords":"","version":null},{"title":"balance: a Python package for adjusting biased samples​","type":1,"pageTitle":"Bringing \"balance\" to your data","url":"/blog/2023/01/09/bringing-balance-to-your-data/#balance-a-python-package-for-adjusting-biased-samples","content":"With survey data playing a key role in research and product work at Meta, we observed a growing need for software tools that make survey statistics methods accessible for researchers and engineers. This has led us to develop “balance”: A Python package for adjusting biased data samples. In balance we introduce a simple easy-to-use framework for weighting data and evaluating its biases with and without adjustments. The package is designed to provide best practices for weights fitting and offers several modeling approaches. The methodology in “balance” already supports ongoing automated survey data processing at Meta, as well as ad-hoc analyses of survey data by dozens of researchers every month. The main workflow API of balance includes three steps: (1) understanding the initial bias in the data relative to a target we would like to infer, (2) adjusting the data to correct for the bias by producing weights for each unit in the sample based on propensity scores, and (3) evaluating the final bias and the variance inflation after applying the fitted weights. The adjustment step provides several alternatives for the researcher to choose from. Current options include: Inverse propensity weighting of the form of logistic regression model based on LASSO (Least Absolute Shrinkage and Selection Operator [1]), Covariate Balancing Propensity Scores [2], and post-stratification. The focus is on providing a simple to use API, based on Pandas DataFrame structure, that can be used by researchers from a wide spectrum of fields. We’re releasing “balance” as a Meta Open Source project. We want researchers, data scientists, engineers, and other practitioners to be able to apply these practices when they work in Python, benefiting from Meta’s long research and experience in the field. With relation to “balance” we hope to also create an active community of data science practitioners where people can come together to discuss methodology and build tools that benefit survey-based research across academia and industry. If you work in Python with potentially biased data, we encourage you to use “balance” in your project. “balance“ website: https://import-balance.org/ github repository: https://github.com/facebookresearch/balance References [1] Tibshirani, R. (1996). Regression shrinkage and selection via the lasso. Journal of the Royal Statistical Society: Series B (Methodological), 58(1), 267-288. [2] Imai, K., &amp; Ratkovic, M. (2014). Covariate balancing propensity score. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 76(1), 243-263. ","version":null,"tagName":"h2"},{"title":"Balance in motion - v0.12.0 expands Python version support, visualizations, and statistical methods","type":0,"sectionRef":"#","url":"/blog/2025/10/25/balance-0-12-0/","content":"","keywords":"","version":null},{"title":"Cutting-Edge Python Compatibility​","type":1,"pageTitle":"Balance in motion - v0.12.0 expands Python version support, visualizations, and statistical methods","url":"/blog/2025/10/25/balance-0-12-0/#cutting-edge-python-compatibility","content":"balance now supports all three major OS platforms: Windows, macOS, and Linux - for Python 3.9 through 3.14, ensuring you can use the latest Python features without compatibility concerns. ","version":null,"tagName":"h2"},{"title":"What's New:​","type":1,"pageTitle":"Balance in motion - v0.12.0 expands Python version support, visualizations, and statistical methods","url":"/blog/2025/10/25/balance-0-12-0/#whats-new","content":"Full support for WindowsFull support for Python 3.11, 3.12, 3.13 and 3.14Smart dependency management with version-specific constraints for numpy, pandas, scipy, and scikit-learn for Python 3.9-3.11.Greater flexibility for Python 3.12+ users with removed upper version constraints, while eliminating 260+ pandas deprecation warnings and modernized our code.Python 3.8 deprecated due to typing incompatibilitiesLicense Update from GPL v2 to the MIT license for greater flexibility and easier integration into your projects ","version":null,"tagName":"h3"},{"title":"Methodological Improvements​","type":1,"pageTitle":"Balance in motion - v0.12.0 expands Python version support, visualizations, and statistical methods","url":"/blog/2025/10/25/balance-0-12-0/#methodological-improvements","content":"","version":null,"tagName":"h2"},{"title":"Transition to scikit-learn for IPW​","type":1,"pageTitle":"Balance in motion - v0.12.0 expands Python version support, visualizations, and statistical methods","url":"/blog/2025/10/25/balance-0-12-0/#transition-to-scikit-learn-for-ipw","content":"We've migrated from glmnet to scikit-learn's logistic regression (v0.10.0) for our inverse propensity weighting (IPW) method, bringing significant benefits: Pros: Windows OS supportPython 3.11+ compatibilityEliminated glmnet dependency Trade-offs: Uses L2 penalties instead of L1 (slight weight differences)2-5x slower than previous version ","version":null,"tagName":"h3"},{"title":"Raking Algorithm: Faster and More Reliable​","type":1,"pageTitle":"Balance in motion - v0.12.0 expands Python version support, visualizations, and statistical methods","url":"/blog/2025/10/25/balance-0-12-0/#raking-algorithm-faster-and-more-reliable","content":"We've completely refactored our raking (rake weighting) implementation with an array-based IPFN algorithm that delivers: Support for marginal distribution target distributions with the new prepare_marginal_dist_for_raking helper functionBetter performance across all Python versionsConsistent results through automatic variable alphabetization ","version":null,"tagName":"h3"},{"title":"Flexibility with Poststratification​","type":1,"pageTitle":"Balance in motion - v0.12.0 expands Python version support, visualizations, and statistical methods","url":"/blog/2025/10/25/balance-0-12-0/#flexibility-with-poststratification","content":"The poststratify method now includes a strict_matching parameter (default True). When set to False, it gracefully handles missing sample cells by issuing warnings and assigning zero weights. ","version":null,"tagName":"h3"},{"title":"Visualization and Summarization Enhancements​","type":1,"pageTitle":"Balance in motion - v0.12.0 expands Python version support, visualizations, and statistical methods","url":"/blog/2025/10/25/balance-0-12-0/#visualization-and-summarization-enhancements","content":"","version":null,"tagName":"h2"},{"title":"Interactive Plotting​","type":1,"pageTitle":"Balance in motion - v0.12.0 expands Python version support, visualizations, and statistical methods","url":"/blog/2025/10/25/balance-0-12-0/#interactive-plotting","content":"All visualization functions now produce interactive Plotly plots by default: Customizable layouts via kwargs (control width, height, and more)New plotly_plot_density for interactive kernel density estimation with support for 'kde' plots in plotly_plot_dist and plot_distBalanceWeightsDF.plot now uses Plotly instead of static seaborn plots  All bar plots now support ylim argument for precise y-axis control: s3_null.covars().plot(ylim=(0, 1)) ","version":null,"tagName":"h3"},{"title":"Statistical Summaries​","type":1,"pageTitle":"Balance in motion - v0.12.0 expands Python version support, visualizations, and statistical methods","url":"/blog/2025/10/25/balance-0-12-0/#statistical-summaries","content":"New variance and confidence interval methods make it easier to assess uncertainty: .var_of_mean() - Variance of weighted means.ci_of_mean() - Confidence intervals for weighted means.mean_with_ci() - Combined mean with confidence intervalsEnhanced .summary() method for BalanceWeightsDF ","version":null,"tagName":"h3"},{"title":"Developer Experience​","type":1,"pageTitle":"Balance in motion - v0.12.0 expands Python version support, visualizations, and statistical methods","url":"/blog/2025/10/25/balance-0-12-0/#developer-experience","content":"","version":null,"tagName":"h2"},{"title":"Notable Bug Fixes​","type":1,"pageTitle":"Balance in motion - v0.12.0 expands Python version support, visualizations, and statistical methods","url":"/blog/2025/10/25/balance-0-12-0/#notable-bug-fixes","content":"Fixed rm_mutual_nas to preserve Series indexImproved Sample.from_frame weight column detection (now recognizes &quot;weights&quot; and &quot;weight&quot;)Better handling of int8/int16 columns (converts to float16)Fixed color assignments in comparison plotsResolved various edge cases in plot_hist_kde and plot_bar ","version":null,"tagName":"h3"},{"title":"CLI Improvements​","type":1,"pageTitle":"Balance in motion - v0.12.0 expands Python version support, visualizations, and statistical methods","url":"/blog/2025/10/25/balance-0-12-0/#cli-improvements","content":"The command-line interface now offers more control: Formula specification via string argumentsType standardization controlsOriginal dtype preservation with --return_df_with_original_dtypesFlexible trimming with weight_trimming_mean_ratio=None optionEnhanced logging with dtype change warnings ","version":null,"tagName":"h3"},{"title":"Documentation & Tutorials​","type":1,"pageTitle":"Balance in motion - v0.12.0 expands Python version support, visualizations, and statistical methods","url":"/blog/2025/10/25/balance-0-12-0/#documentation--tutorials","content":"We've significantly expanded our documentation with new tutorials: Quickstart - Get started with balance basicsQuickstart with Raking - Compare raking vs. IPWQuickstart with CBPS - Covariate Balancing Propensity Score methodTransformations and Formulas - Advanced covariate preprocessingCBPS: R vs. Python Comparison - Validation against R's CBPS package Also added a Link to conference presentations (ISA 2023). This extends our existing Statistical Methods Documentation: Inverse Propensity Weighting (IPW)Covariate Balancing Propensity Score (CBPS)Post-stratificationRaking ","version":null,"tagName":"h2"},{"title":"Community & Contributors​","type":1,"pageTitle":"Balance in motion - v0.12.0 expands Python version support, visualizations, and statistical methods","url":"/blog/2025/10/25/balance-0-12-0/#community--contributors","content":"A huge thank you to our contributors: @talgalili, @wesleytlee, @SarigT, @ahakso, @stevemandala, @tomwagstaff-opml, @zbraiterman, and @luca-martial! Want to contribute? Check out our contributing guide.  ","version":null,"tagName":"h2"},{"title":"Get Started Today​","type":1,"pageTitle":"Balance in motion - v0.12.0 expands Python version support, visualizations, and statistical methods","url":"/blog/2025/10/25/balance-0-12-0/#get-started-today","content":"Ready to try balance or upgrade to v0.12.0? ","version":null,"tagName":"h2"},{"title":"Installation:​","type":1,"pageTitle":"Balance in motion - v0.12.0 expands Python version support, visualizations, and statistical methods","url":"/blog/2025/10/25/balance-0-12-0/#installation","content":"python -m pip install balance  ","version":null,"tagName":"h3"},{"title":"Resources:​","type":1,"pageTitle":"Balance in motion - v0.12.0 expands Python version support, visualizations, and statistical methods","url":"/blog/2025/10/25/balance-0-12-0/#resources","content":"Website: https://import-balance.org/GitHub: https://github.com/facebookresearch/balanceDocumentation: https://import-balance.org/docs/docs/general_framework/Tutorials: https://import-balance.org/docs/tutorials/Blog: https://import-balance.org/blog/Paper: balance – a Python package for balancing biased data samples (Sarig, Galili, &amp; Eilat, 2023) ","version":null,"tagName":"h3"},{"title":"Get Help:​","type":1,"pageTitle":"Balance in motion - v0.12.0 expands Python version support, visualizations, and statistical methods","url":"/blog/2025/10/25/balance-0-12-0/#get-help","content":"Ask questions: https://github.com/facebookresearch/balance/issues/new?template=support_question.mdReport bugs: https://github.com/facebookresearch/balance/issues/new?template=bug_report.mdRequest features: https://github.com/facebookresearch/balance/issues/new?template=feature_request.md We welcome your feedback, questions, and contributions as we continue making balance the go-to tool for survey statistics and bias adjustment in Python! ","version":null,"tagName":"h3"},{"title":"Contributing","type":0,"sectionRef":"#","url":"/docs/docs/contributing/","content":"","keywords":"","version":"Next"},{"title":"Pull Requests​","type":1,"pageTitle":"Contributing","url":"/docs/docs/contributing/#pull-requests","content":"We actively welcome your pull requests. Fork the repo and create your branch from main.If you've added code that should be tested, add tests.If you've changed APIs, update the documentation.Ensure the test suite passes.Make sure your code lints.If you haven't already, complete the Contributor License Agreement (&quot;CLA&quot;). ","version":"Next","tagName":"h2"},{"title":"Contributor License Agreement (\"CLA\")​","type":1,"pageTitle":"Contributing","url":"/docs/docs/contributing/#contributor-license-agreement-cla","content":"In order to accept your pull request, we need you to submit a CLA. You only need to do this once to work on any of Meta's open source projects. Complete your CLA here: https://code.facebook.com/cla ","version":"Next","tagName":"h2"},{"title":"Issues​","type":1,"pageTitle":"Contributing","url":"/docs/docs/contributing/#issues","content":"We use GitHub issues to track public bugs. Please ensure your description is clear and has sufficient instructions to be able to reproduce the issue. Meta has a bounty program for the safe disclosure of security bugs. In those cases, please go through the process outlined on that page and do not file a public issue. ","version":"Next","tagName":"h2"},{"title":"Code Requirements​","type":1,"pageTitle":"Contributing","url":"/docs/docs/contributing/#code-requirements","content":"","version":"Next","tagName":"h2"},{"title":"Coding Style​","type":1,"pageTitle":"Contributing","url":"/docs/docs/contributing/#coding-style","content":"4 spaces for indentation rather than tabs88 character line length ","version":"Next","tagName":"h3"},{"title":"Linting and Code Formatting​","type":1,"pageTitle":"Contributing","url":"/docs/docs/contributing/#linting-and-code-formatting","content":"We use ufmt (µfmt) for code formatting and flake8 for linting. This matches Meta's internal pyfmt tooling and ensures consistency between internal and external development. Installation: pip install -r requirements-fmt.txt  Running formatters and linters: # Check formatting (doesn't modify files) ufmt check . # Auto-format code ufmt format . # Run flake8 linter flake8 .  Pre-commit hooks (optional but recommended): pip install pre-commit pre-commit install  After installing pre-commit hooks, formatting and linting will run automatically on every commit. Configuration files: .flake8 - Custom flake8 configurationpyproject.toml - Configuration for ufmt, black, and usortrequirements-fmt.txt - Pinned versions of formatting tools matching Meta's internal setup ","version":"Next","tagName":"h3"},{"title":"Static Type Checking​","type":1,"pageTitle":"Contributing","url":"/docs/docs/contributing/#static-type-checking","content":"We use Pyre for static type checking and require code to be fully type annotated. ","version":"Next","tagName":"h3"},{"title":"Unit testing​","type":1,"pageTitle":"Contributing","url":"/docs/docs/contributing/#unit-testing","content":"We strongly recommend adding unit testing when introducing new code. To run all unit tests, we recommend installing pytest using pip install pytest and running pytest -ra from the root of the balance repo. ","version":"Next","tagName":"h3"},{"title":"Documentation​","type":1,"pageTitle":"Contributing","url":"/docs/docs/contributing/#documentation","content":"We require docstrings on all public functions and classes (those not prepended with _).We use the Google docstring style &amp; use Sphinx to compile API reference documentation.Our website leverages Docusaurus 2.0 + Sphinx + Jupyter notebook for generating our documentation content.To rule out parsing errors, we suggesting installing sphinx and running make html from the balance/sphinx folder. ","version":"Next","tagName":"h3"},{"title":"Website Development​","type":1,"pageTitle":"Contributing","url":"/docs/docs/contributing/#website-development","content":"","version":"Next","tagName":"h2"},{"title":"Overview​","type":1,"pageTitle":"Contributing","url":"/docs/docs/contributing/#overview","content":"balance's website is also open source and part of this repository. balance leverages several open source frameworks for website development. Docusaurus 2: The main site is generated using Docusaurus, with the code living under the website folder). This includes the website template (navbar, footer, sidebars), landing page, and main sections (Blog, Docs, Tutorials, API Reference). Markdown is used for the content of several sections, particularly the &quot;Docs&quot; section. Files are under the docs/ folder Jupyter notebook is used to generate the notebook tutorials under the &quot;Tutorials&quot; section, based on our ipynb tutorials in our tutorials folder.Sphinx is used for Python documentation generation, populated under the &quot;API Reference&quot; section. Files are under the sphinx folder. ","version":"Next","tagName":"h3"},{"title":"Setup​","type":1,"pageTitle":"Contributing","url":"/docs/docs/contributing/#setup","content":"To install the necessary dependencies for website development, run the following from the repo root: python -m pip install git+https://github.com/bbalasub1/glmnet_python.git@1.0 python -m pip install .[dev]  ","version":"Next","tagName":"h3"},{"title":"Adding Notebook Tutorials​","type":1,"pageTitle":"Contributing","url":"/docs/docs/contributing/#adding-notebook-tutorials","content":"All our notebook tutorials are housed under the tutorials folder at the root of the repo. We use these notebooks as the source of truth for the &quot;Tutorials&quot; section of the website, executing &amp; generating HTML pages for each notebook. To add a new tutorial: Check in your notebook (.ipynb) to our tutorials folder. We strongly suggest clearing notebook output cells.Extend the &quot;Building tutorial HTML&quot; section of scripts/make_docs.sh to execute &amp; generate HTML for the new tutorial e.g. jupyter nbconvert tutorials/my_tutorial.ipynb --execute --to html --output-dir website/static/html/tutorials.Introduce a new .mdx page under the website/docs/tutorials folder for the new tutorial. Use HTMLLoader to load the generated HTML e.g. &lt;HTMLLoader docFile={useBaseUrl('html/tutorials/my_tutorial.html')}/&gt;. quickstart.mdx is a good reference for the setup To test the setup, see the Building &amp; Testing Website Changes section below. Note: The generated HTML should not be checked into the main repo. ","version":"Next","tagName":"h3"},{"title":"Building & Testing Website Changes​","type":1,"pageTitle":"Contributing","url":"/docs/docs/contributing/#building--testing-website-changes","content":"We've developed a helper script for running the full website build process: ./scripts/make_docs.sh # To start up the local webserver cd website yarn serve  Once the local webserver is up, you'll get a link you can follow to visit the newly-built site. See Docusaurus docs for more info. ","version":"Next","tagName":"h3"},{"title":"Deployment​","type":1,"pageTitle":"Contributing","url":"/docs/docs/contributing/#deployment","content":"We rely on Github Actions to run our CI/CD. The workflow files can be found here. In summary On every pull request, we run our &quot;Build &amp; Test&quot; workflow, which includes PyTest tests, Wheels package builds, flake8 linting, and website build.We also run the same &quot;Build &amp; Test&quot; suite nightly.On every push, we deploy a new version of the website. The make_docs.sh script is run from the main branch and the build artifacts are published to the gh-pages branch, which is linked to our repo's Github Page's deployment. ","version":"Next","tagName":"h2"},{"title":"Releasing a new version​","type":1,"pageTitle":"Contributing","url":"/docs/docs/contributing/#releasing-a-new-version","content":"To create a new release, simply navigate to the &quot;Release&quot; page of the repo, draft a new release, and publish. The Github Action workflow should be triggered on publish and you should see a new version of the package live on PyPi in ~10 mins. You can check the status of the job via the GH Actions tab. Guidelines when drafting a new release: Follow semantic versioning conventions when chosing the next version.The release's tag should only be the version itself (e.g. &quot;0.1.0&quot;). Do not add any prefixes like &quot;v&quot; or &quot;version&quot;. The build process relies on proper formatting of this tag. The Github Actions job is configured at release.yml. ","version":"Next","tagName":"h3"},{"title":"License​","type":1,"pageTitle":"Contributing","url":"/docs/docs/contributing/#license","content":"By contributing to balance, you agree that your contributions will be licensed under the LICENSE file in the root directory of this source tree. ","version":"Next","tagName":"h2"},{"title":"General Framework","type":0,"sectionRef":"#","url":"/docs/docs/general_framework/","content":"","keywords":"framework","version":"Next"},{"title":"References​","type":1,"pageTitle":"General Framework","url":"/docs/docs/general_framework/#references","content":"[1] Salganik, Matthew J. 2017. Bit by Bit: Social Research in the Digital Age. Princeton, NJ: Princeton University Press. Open review edition. ","version":"Next","tagName":"h2"},{"title":"Balance v0.15.0 - more sklearn models, diagnostics, and stability (100% test-coverage!)","type":0,"sectionRef":"#","url":"/blog/2026/01/20/balance-0-15-0/","content":"","keywords":"","version":null},{"title":"What is balance?​","type":1,"pageTitle":"Balance v0.15.0 - more sklearn models, diagnostics, and stability (100% test-coverage!)","url":"/blog/2026/01/20/balance-0-15-0/#what-is-balance","content":"balance is a Python package (from Meta) offering a simple workflow and methods for dealing with biased data samples when looking to infer from them to some population of interest. Biased samples often occur in survey statistics when respondents present non-response bias or surveys suffer from sampling bias (that are not missing completely at random). A similar issue arises in observational studies when comparing treated vs untreated groups, and in any data that suffers from selection bias. ","version":null,"tagName":"h3"},{"title":"Highlights from v0.15.0 (since v0.12.0):​","type":1,"pageTitle":"Balance v0.15.0 - more sklearn models, diagnostics, and stability (100% test-coverage!)","url":"/blog/2026/01/20/balance-0-15-0/#highlights-from-v0150-since-v0120","content":"✅ More control over modeling: The ability to run any sklearn model (instead of just LogisticRegression) to fit inverse-propensity-score weights. Plus formula-driven summaries and explicit missing-data handling. ✅ Stronger diagnostics: The way weights influence covariate imbalance can now be evaluated not just with ASMD (as before), but also with various distribution distance metrics (KLD, EMD, CVMD, KS). ✅ Reliable code: Test coverage was increased to 100%, with full type-checking across the whole codebase. Plus CLI enhancements and improved docs/tutorials.  ","version":null,"tagName":"h3"},{"title":"Updated Tutorials​","type":1,"pageTitle":"Balance v0.15.0 - more sklearn models, diagnostics, and stability (100% test-coverage!)","url":"/blog/2026/01/20/balance-0-15-0/#updated-tutorials","content":"Post-stratification tutorial notebook (and expanded documentation): https://import-balance.org/docs/tutorials/quickstart_poststratify/CLI tutorial: https://import-balance.org/docs/tutorials/balance_cli_tutorial/Customizing IPW models: https://import-balance.org/docs/tutorials/quickstart/ ","version":null,"tagName":"h2"},{"title":"More Flexible IPW Modeling and Summary Tools​","type":1,"pageTitle":"Balance v0.15.0 - more sklearn models, diagnostics, and stability (100% test-coverage!)","url":"/blog/2026/01/20/balance-0-15-0/#more-flexible-ipw-modeling-and-summary-tools","content":"","version":null,"tagName":"h2"},{"title":"Bring Your Own IPW sklearn Estimator​","type":1,"pageTitle":"Balance v0.15.0 - more sklearn models, diagnostics, and stability (100% test-coverage!)","url":"/blog/2026/01/20/balance-0-15-0/#bring-your-own-ipw-sklearn-estimator","content":".adjust(method=&quot;ipw&quot;) now accepts any scikit-learn classifier via the model argument, so you can use estimators like random forests or gradient boosting. You can also pass a configured LogisticRegression instance or provide JSON-configured parameters through the CLI. Examples below assume sample and target are Sample objects unless otherwise noted. Example setup used by multiple snippets: import pandas as pd from balance.sample_class import Sample sample_df = pd.DataFrame( { &quot;id&quot;: [1, 2, 3, 4], &quot;age&quot;: [20, 30, 40, 50], &quot;group&quot;: [&quot;A&quot;, &quot;B&quot;, &quot;A&quot;, &quot;B&quot;], &quot;outcome&quot;: [1.0, 2.0, 3.0, 4.0], &quot;weight&quot;: [1.0, 1.0, 1.0, 1.0], } ) target_df = pd.DataFrame( { &quot;id&quot;: [10, 11, 12, 13, 14, 15], &quot;age&quot;: [25, 35, 45, 55, 65, 75], &quot;group&quot;: [&quot;A&quot;, &quot;B&quot;, &quot;A&quot;, &quot;B&quot;, &quot;A&quot;, &quot;B&quot;], &quot;outcome&quot;: [1.5, 2.5, 3.5, 4.5, 5.5, 6.5], &quot;weight&quot;: [1.0] * 6, } ) sample = Sample.from_frame( sample_df, id_column=&quot;id&quot;, weight_column=&quot;weight&quot;, outcome_columns=[&quot;outcome&quot;], standardize_types=False, ) target = Sample.from_frame( target_df, id_column=&quot;id&quot;, weight_column=&quot;weight&quot;, outcome_columns=[&quot;outcome&quot;], standardize_types=False, ) sample.df  Output:  id age group outcome weight 0 1 20 A 1.0 1.0 1 2 30 B 2.0 1.0 2 3 40 A 3.0 1.0 3 4 50 B 4.0 1.0  Example: from sklearn.ensemble import RandomForestClassifier rf = RandomForestClassifier(random_state=0) adjusted = sample.adjust(target, method=&quot;ipw&quot;, model=rf)  A detailed example is given here: https://import-balance.org/docs/tutorials/quickstart/ ","version":null,"tagName":"h3"},{"title":"Diagnostics That Go Further​","type":1,"pageTitle":"Balance v0.15.0 - more sklearn models, diagnostics, and stability (100% test-coverage!)","url":"/blog/2026/01/20/balance-0-15-0/#diagnostics-that-go-further","content":"","version":null,"tagName":"h2"},{"title":"Beyond ASMD: New Distance Metrics​","type":1,"pageTitle":"Balance v0.15.0 - more sklearn models, diagnostics, and stability (100% test-coverage!)","url":"/blog/2026/01/20/balance-0-15-0/#beyond-asmd-new-distance-metrics","content":"Balance now exposes KL divergence, Earth Mover's Distance (EMD), Cramér-von Mises distance (CVMD), and Kolmogorov–Smirnov (KS) statistics through BalanceDF diagnostics. These diagnostics work with weighted or unweighted comparisons, include discrete/continuous variants, and respect one-hot categorical aggregation when enabled. Example: # Compare covariate distributions between a sample and target. sample.set_target(target).covars().kld() sample.set_target(target).covars().emd() sample.set_target(target).covars().cvmd() sample.set_target(target).covars().ks()  Output:  age group[A] group[B] mean(kld) index covars 0.232348 0.0 0.0 0.116174 age group[A] group[B] mean(emd) index covars 15.0 0.0 0.0 7.5 age group[A] group[B] mean(cvmd) index covars 0.083333 0.0 0.0 0.041667 age group[A] group[B] mean(ks) index covars 0.5 0.0 0.0 0.25  ","version":null,"tagName":"h3"},{"title":"Richer Adjusted Sample Summaries​","type":1,"pageTitle":"Balance v0.15.0 - more sklearn models, diagnostics, and stability (100% test-coverage!)","url":"/blog/2026/01/20/balance-0-15-0/#richer-adjusted-sample-summaries","content":"Adjusted samples now surface more information at a glance: Sample.__str__() shows adjustment method, trimming parameters, design effect, and effective sample size.Sample.summary() groups covariate diagnostics, reports design effect alongside ESSP/ESS, and surfaces weighted outcome means when available. Example: adjusted = sample.adjust(target, method=&quot;ipw&quot;) adjusted.summary()  Output: Adjustment details: method: ipw weight trimming mean ratio: 20 Covariate diagnostics: Covar ASMD reduction: -3.0% Covar ASMD (3 variables): 0.401 -&gt; 0.413 Covar mean KLD reduction: 2.2% Covar mean KLD (2 variables): 0.116 -&gt; 0.114 Weight diagnostics: design effect (Deff): 1.001 effective sample size proportion (ESSP): 0.999 effective sample size (ESS): 4.0 Outcome weighted means: outcome source self 2.479 unadjusted 2.500 target 4.000 Model performance: Model proportion deviance explained: 0.034  ","version":null,"tagName":"h3"},{"title":"Smarter Weighting Workflows​","type":1,"pageTitle":"Balance v0.15.0 - more sklearn models, diagnostics, and stability (100% test-coverage!)","url":"/blog/2026/01/20/balance-0-15-0/#smarter-weighting-workflows","content":"","version":null,"tagName":"h2"},{"title":"CLI Now Supports Outcome Columns​","type":1,"pageTitle":"Balance v0.15.0 - more sklearn models, diagnostics, and stability (100% test-coverage!)","url":"/blog/2026/01/20/balance-0-15-0/#cli-now-supports-outcome-columns","content":"The CLI now supports --outcome_columns, letting you explicitly declare which columns are outcomes. Remaining columns are moved to ignored_columns instead of being treated implicitly. Example: from argparse import Namespace from balance.cli import BalanceCLI BalanceCLI(Namespace(outcome_columns=&quot;y,z&quot;)).outcome_columns()  Output: ['y', 'z']  ","version":null,"tagName":"h3"},{"title":"High-Cardinality Covariate Warnings​","type":1,"pageTitle":"Balance v0.15.0 - more sklearn models, diagnostics, and stability (100% test-coverage!)","url":"/blog/2026/01/20/balance-0-15-0/#high-cardinality-covariate-warnings","content":"Balance warns when categorical covariates have &gt;=80% unique values (e.g., user IDs), helping identify problematic columns before fitting. Example: # If user_id is high-cardinality, adjust will emit a warning before fitting. import pandas as pd from balance.utils.pandas_utils import _detect_high_cardinality_features _detect_high_cardinality_features( pd.DataFrame({&quot;id&quot;: [&quot;a&quot;, &quot;b&quot;, &quot;c&quot;], &quot;group&quot;: [&quot;a&quot;, &quot;a&quot;, &quot;b&quot;]}), threshold=0.8, )  Output: [HighCardinalityFeature(column='id', unique_count=3, unique_ratio=1.0, has_missing=np.False_)]  ","version":null,"tagName":"h3"},{"title":"Developer Improvements, Bug Fixes & Breaking Changes​","type":1,"pageTitle":"Balance v0.15.0 - more sklearn models, diagnostics, and stability (100% test-coverage!)","url":"/blog/2026/01/20/balance-0-15-0/#developer-improvements-bug-fixes--breaking-changes","content":"README badges for build status, version support, release tracking, and unittest coverage: https://import-balance.org/docs/docs/overview/ poststratify() now supports na_action to either drop missing rows or treat missing values as their own category; breaking change: missing values default to a &quot;__NaN__&quot; category, so legacy drop behavior requires na_action=&quot;drop&quot;. import pandas as pd from balance.weighting_methods.poststratify import poststratify sample_df = pd.DataFrame({&quot;gender&quot;: [&quot;Female&quot;, None, &quot;Male&quot;, &quot;Female&quot;]}) target_df = pd.DataFrame({&quot;gender&quot;: [&quot;Female&quot;, None, None, &quot;Male&quot;]}) poststratify( sample_df=sample_df, sample_weights=pd.Series([1, 1, 1, 1]), target_df=target_df, target_weights=pd.Series([1, 1, 1, 1]), variables=[&quot;gender&quot;], na_action=&quot;add_indicator&quot;, )[&quot;weight&quot;].tolist()  [0.5, 2.0, 1.0, 0.5]  model_matrix(add_na=False) now actually drops rows with missing values while preserving categorical levels (matching the documented behavior, not just warning): import pandas as pd from balance.utils.model_matrix import model_matrix df = pd.DataFrame({&quot;age&quot;: [20, None, 30], &quot;group&quot;: [&quot;A&quot;, &quot;B&quot;, &quot;A&quot;]}) model_matrix(df, add_na=False)[&quot;sample&quot;]   age group[A] group[B] 0 20.0 1.0 0.0 2 30.0 1.0 0.0  Under the hood, developers get: trimming parity for rake()/poststratify() via trim_weights(..., target_sum_weights=...), warnings for very large targets (&gt;=100k and &gt;=10× sample), more consistent percentile trimming via explicit clipping, formula-driven summaries in descriptive_stats(formula=...), consolidated diagnostics helpers (and breaking change: IPW Sample.diagnostics() output shape now always includes iteration/intercept summaries plus hyperparameters), and a split of the old balance.util into focused balance.utils submodules. Testing/typing updates include 100% coverage(!), migrating 32 files to # pyre-strict, modernized PEP 604 type hints, TypedDict definitions for plotting, renaming Balance_* classes to BalanceDF variants, adding Pyre checking in GitHub Actions via .pyre_configuration.external, and aligning formatting/CI tooling with Black 25.1.0. The raking algorithm was refactored to remove the ipfn dependency in favor of a vectorized NumPy implementation. Bug fixes (v0.13–v0.15) include: stable CBPS probability computation to avoid overflow, honoring weighted=False for target data in categorical QQ plots, earlier validation errors for null weights in Sample.from_frame, and trim_weights() now returning a consistent float64 Series while preserving index ordering. Breaking changes to watch when upgrading: poststratify() defaults to &quot;__NaN__&quot; missing-category handling (use na_action=&quot;drop&quot; to drop), model_matrix(add_na=False) drops missing-data rows, percentile trimming uses explicit clipping bounds (thresholds may shift by ~1 observation), and IPW Sample.diagnostics() output shape changed to always include iteration/intercept summaries and hyperparameter settings. Details are in: https://import-balance.org/docs/docs/changelog/ ","version":null,"tagName":"h2"},{"title":"Community & Contributors​","type":1,"pageTitle":"Balance v0.15.0 - more sklearn models, diagnostics, and stability (100% test-coverage!)","url":"/blog/2026/01/20/balance-0-15-0/#community--contributors","content":"A huge thank you to everyone who contributed to versions 0.13–0.15, including @neuralsorcerer, @talgalili, @wesleytlee, the BPG team in Tel-Aviv, and the broader community! Want to contribute? Check out our contributing guide. ","version":null,"tagName":"h2"},{"title":"Get Started with v0.15.0​","type":1,"pageTitle":"Balance v0.15.0 - more sklearn models, diagnostics, and stability (100% test-coverage!)","url":"/blog/2026/01/20/balance-0-15-0/#get-started-with-v0150","content":"Upgrade today: python -m pip install -U balance  Resources: Website: https://import-balance.org/GitHub: https://github.com/facebookresearch/balanceDocumentation: https://import-balance.org/docs/docs/general_framework/Tutorials: https://import-balance.org/docs/tutorials/Blog: https://import-balance.org/blog/Paper: balance – a Python package for balancing biased data samples Need help? Ask questions: https://github.com/facebookresearch/balance/issues/new?template=support_question.mdReport bugs: https://github.com/facebookresearch/balance/issues/new?template=bug_report.mdRequest features: https://github.com/facebookresearch/balance/issues/new?template=feature_request.md ","version":null,"tagName":"h2"},{"title":"Adjusting Sample to Population","type":0,"sectionRef":"#","url":"/docs/docs/general_framework/adjusting_sample_to_population/","content":"","keywords":"adjustment","version":"Next"},{"title":"Optional arguments​","type":1,"pageTitle":"Adjusting Sample to Population","url":"/docs/docs/general_framework/adjusting_sample_to_population/#optional-arguments","content":"method: ipw, poststratify, rake, or cbps. Default is ipw. ipw: stands for Inverse Propensity Weighting. The propensity scores are calculated with LASSO logistic regression. Details about the implementation can be found here. For a quick-start tutorial, see here.cbps: stands for Covariate Balancing Propensity Score. The CBPS algorithm estimates the propensity score in a way that optimizes prediction of the probability of sample inclusion as well as the covariates balance. Its main advantage is in cases when the researcher wants better balance on the covariates than traditional propensity score methods - because one believes the assignment model might be misspecified and would like to avoid an iterative procedure of balancing the covariates. Details about the implementation can be found here. For a quick-start tutorial, see here.poststratify: stands for post-stratification. Details about the implementation can be found here.rake: Details about the implementation can be found here. For a quick-start tutorial, see here. variables: allows user to pass a list of the covariates that they want to adjust for; if variables argument is not specified, all joint variables in sample and target are used. transformations: which transformations to apply to data before fitting the model. Default is cutting numeric variables into 10 quantile buckets and lumping together infrequent levels with less than 5% prevalence into lumped_other category. The transformations are done on both the sample dataframe and the target dataframe together. User can also specify specific transformations in a dictionary format. For a quick-start tutorial on transformations and formulas, see here. max_de: (for ipw and cbps methods): The default value is 1.5. It limits the design effect to be within 1.5. If set to None, the optimization is performed by cross-validation of the logistic model for ipw (see the choose_regularization function for more details) or without constrained optimization for cbps. Setting max_de to None can sometimes significantly improve the running time of the code. weight_trimming_mean_ratio or weight_trimming_percentile: (only one of these arguments can be specified). weight_trimming_mean_ratio indicates the ratio from above according to which the weights are trimmed by mean(weights) * ratio. Default is 20. If weight_trimming_percentile is not none, winsorization is applied. Default is None, i.e. trimming from above is applied. However, note that when max_de is not None (and default is 1.5), the trimming-ratio is optimized by ipw and these arguments are ignored. na_action (for ipw method): how to handle missing values in the data (sample and target). Default is to replace NAs with 0's and add indicator for which observations were NA (this is done after applying the transformations). Another option is drop, which drops all observations with NA values. formula (for ipw and cbps methods): The formula according to which build the model matrix for the logistic regression. Default is a linear additive formula of all covariates. For a quick-start tutorial on transformations and formulas, see here. penalty_factor (for ipw method): the penalty used in the regularized logistic regression. ","version":"Next","tagName":"h2"},{"title":"0.16.0 (Unreleased)","type":0,"sectionRef":"#","url":"/docs/docs/changelog/","content":"","keywords":"","version":"Next"},{"title":"New Features​","type":1,"pageTitle":"0.16.0 (Unreleased)","url":"/docs/docs/changelog/#new-features","content":"Validate weights include positive values Added a guard in weight diagnostics to error when all weights are zero. ","version":"Next","tagName":"h2"},{"title":"Bug Fixes​","type":1,"pageTitle":"0.16.0 (Unreleased)","url":"/docs/docs/changelog/#bug-fixes","content":"Removed deprecated setup build Replaced deprecated setup.py with pyproject.toml build in CI to avoid build failure. ","version":"Next","tagName":"h2"},{"title":"Packaging & Tests​","type":1,"pageTitle":"0.16.0 (Unreleased)","url":"/docs/docs/changelog/#packaging--tests","content":"Pandas 2.x compatibility and upper bound (&lt;3.0.0) Constrained the pandas dependency to &gt;=2,&lt;3.0.0 to avoid untested pandas 3.x API and dtype changes. ","version":"Next","tagName":"h2"},{"title":"Breaking Changes​","type":1,"pageTitle":"0.16.0 (Unreleased)","url":"/docs/docs/changelog/#breaking-changes","content":"Require positive weights for weight diagnostics that normalize or aggregate design_effect, nonparametric_skew, prop_above_and_below, andweighted_median_breakdown_point now raise a ValueError when all weights are zero.Migration: ensure your weights include at least one positive value before calling these diagnostics, or catch the ValueError if all-zero weights are possible in your workflow. 0.15.0 (2026-01-20) ","version":"Next","tagName":"h2"},{"title":"New Features​","type":1,"pageTitle":"0.16.0 (Unreleased)","url":"/docs/docs/changelog/#new-features-1","content":"Added EMD/CVMD/KS distribution diagnostics BalanceDF now exposes Earth Mover's Distance (EMD), Cramér-von Mises distance (CVMD), and Kolmogorov-Smirnov (KS) statistics for comparing adjusted samples to targets.These diagnostics support weighted or unweighted comparisons, apply discrete/continuous formulations, and respect aggregate_by_main_covar for one-hot categorical aggregation. Exposed outcome columns selection in the CLI Added --outcome_columns to choose which columns are treated as outcomes instead of defaulting to all non-id/weight/covariate columns. Remaining columns are moved to ignored_columns. Improved missing data handling in poststratify() poststratify() now accepts na_action to either drop rows with missing values or treat missing values as their own category during weighting.Breaking change: the default behavior now fills missing values in poststratification variables with &quot;__NaN__&quot; and treats this as a distinct category during weighting. Previously, missing values were not handled explicitly, and their treatment depended on pandas groupby and mergedefaults. To approximate the legacy behavior where missing values do not form their own category, pass na_action=&quot;drop&quot; explicitly. Added formula support for descriptive_stats model matrices descriptive_stats() now accepts a formula argument that is always applied to the data (including numeric-only frames), letting callers control which terms and dummy variables are included in summary statistics. ","version":"Next","tagName":"h2"},{"title":"Documentation​","type":1,"pageTitle":"0.16.0 (Unreleased)","url":"/docs/docs/changelog/#documentation","content":"Documented the balance CLI Added full API docstrings for balance.cli and a new CLI tutorial notebook. Created Balance CLI tutorial Added CLI command echoing, a load_data() example, and richer diagnostics exploration with metric/variable listings and a browsable diagnostics table. https://import-balance.org/docs/tutorials/balance_cli_tutorial/ Synchronized docstring examples with test cases Updated user-facing docstrings so the documented examples mirror tested inputs and outputs. ","version":"Next","tagName":"h2"},{"title":"Code Quality & Refactoring​","type":1,"pageTitle":"0.16.0 (Unreleased)","url":"/docs/docs/changelog/#code-quality--refactoring","content":"Added warning when the sample size of 'target' is much larger than 'sample' sample size Sample.adjust() now warns when the target exceeds 100k rows and is at least 10x larger than the sample, highlighting that uncertainty is dominated by the sample (akin to a one-sample comparison). Split util helpers into focused modules Broke balance.util into balance.utils submodules for easier navigation. ","version":"Next","tagName":"h2"},{"title":"Bug Fixes​","type":1,"pageTitle":"0.16.0 (Unreleased)","url":"/docs/docs/changelog/#bug-fixes-1","content":"Updated Sample.__str__() to format weight diagnostics like Sample.summary() Weight diagnostics (design effect, effective sample size proportion, effective sample size) are now displayed on separate lines instead of comma-separated on one line.Replaced &quot;eff.&quot; abbreviations with full &quot;effective&quot; word for better readability.Improves consistency with Sample.summary() output format. Numerically stable CBPS probabilities The CBPS helper now uses a stable logistic transform to avoid exponential overflow warnings during probability computation in constraint checks. Silenced pandas observed default warning Explicitly sets observed=False in weighted categorical KLD calculations to retain current behavior and avoid future pandas default changes. Fixed plot_qq_categorical to respect the weighted parameter for target data Previously, the target weights were always applied regardless of theweighted=False setting, causing inconsistent behavior between sample and target proportions in categorical QQ plots. Restored CBPS tutorial plots Re-enabled scatter plots in the CBPS comparison tutorial notebook while avoiding GitHub Pages rendering errors and pandas colormap warnings. https://import-balance.org/docs/tutorials/comparing_cbps_in_r_vs_python_using_sim_data/ Clearer validation errors in adjustment helpers trim_weights() now accepts list/tuple inputs and reports invalid types explicitly.apply_transformations() raises clearer errors for invalid inputs and empty transformations. Fixed model_matrix to drop NA rows when requested model_matrix(add_na=False) now actually drops rows containing NA values while preserving categorical levels, matching the documented behavior.Previously, add_na=False only logged a warning without dropping rows; code relying on the old behavior may now see fewer rows and should either handle missingness explicitly or use add_na=True. ","version":"Next","tagName":"h2"},{"title":"Tests​","type":1,"pageTitle":"0.16.0 (Unreleased)","url":"/docs/docs/changelog/#tests","content":"Aligned formatting toolchain between Meta internal and GitHub CI Added [&quot;fbcode/core_stats/balance&quot;] override to Meta's internal tools/lint/pyfmt/config.toml to use formatter = &quot;black&quot; and sorter = &quot;usort&quot;.This ensures both internal (pyfmt/arc lint) and external (GitHub Actions) environments use the same Black 25.1.0 formatter, eliminating formatting drift.Updated CI workflow, pre-commit config, and requirements-fmt.txt to use black==25.1.0. Added Pyre type checking to GitHub Actions via .pyre_configuration.external and a new pyre job in the workflow. Tests are excluded due to external typeshed stub differences; library code is fully type-checked.Added test coverage workflow and badge to README via .github/workflows/coverage.yml. The workflow collects coverage using pytest-cov, generates HTML and XML reports, uploads them as artifacts, and displays coverage metrics. A coverage badge is now shown in README.md alongside other workflow badges.Improved test coverage for edge cases and error handling paths Added targeted tests for previously uncovered code paths across the library, addressing edge cases including empty inputs, verbose logging, error handling for invalid parameters, and boundary conditions in weighting methods (IPW, CBPS, rake).Tests exercise defensive code paths that handle empty DataFrames, NaN convergence values, invalid model types, and non-convergence warnings. Split test_util.py into focused test modules Split the large test_util.py file (2325 lines) into 5 modular test files that mirror the balance/utils/ structure: test_util_data_transformation.py - Tests for data transformation utilitiestest_util_input_validation.py - Tests for input validation utilitiestest_util_model_matrix.py - Tests for model matrix utilitiestest_util_pandas_utils.py - Tests for pandas utilities (including high cardinality warnings)test_util_logging_utils.py - Tests for logging utilities This improves test organization and makes it easier to locate tests for specific utilities. ","version":"Next","tagName":"h2"},{"title":"Contributors​","type":1,"pageTitle":"0.16.0 (Unreleased)","url":"/docs/docs/changelog/#contributors","content":"@neuralsorcerer, @talgalili 0.14.0 (2025-12-14) ","version":"Next","tagName":"h2"},{"title":"New Features​","type":1,"pageTitle":"0.16.0 (Unreleased)","url":"/docs/docs/changelog/#new-features-2","content":"Enhanced adjusted sample summary output Sample.__str__() now displays adjustment details (method, trimming parameters, design effect, effective sample size) when printing adjusted samples (#194,#57). Richer Sample.summary() diagnostics Adjusted sample summary now groups covariate diagnostics, reports design effect alongside ESSP/ESS, and surfaces weighted outcome means when available. Warning of high-cardinality categorical features in .adjust() Categorical features where ≥80% of values are unique are flagged before weight fitting to help identify problematic columns like user IDs (#195,#65). Ignored column handling for Sample inputs Sample.from_frame accepts ignore_columns for columns that should remain on the dataframe but be excluded from covariates and outcome statistics. Ignored columns appear in Sample.df and can be retrieved viaSample.ignored_columns(). ","version":"Next","tagName":"h2"},{"title":"Code Quality & Refactoring​","type":1,"pageTitle":"0.16.0 (Unreleased)","url":"/docs/docs/changelog/#code-quality--refactoring-1","content":"Consolidated diagnostics helpers Added _concat_metric_val_var() helper and balance.util._coerce_scalarfor robust diagnostics row construction and scalar-to-float conversion.Breaking change: Sample.diagnostics() for IPW now always emits iteration/intercept summaries plus hyperparameter settings. ","version":"Next","tagName":"h2"},{"title":"Bug Fixes​","type":1,"pageTitle":"0.16.0 (Unreleased)","url":"/docs/docs/changelog/#bug-fixes-2","content":"Early validation of null weight inputs Sample.from_frame now raises ValueError when weights contain None,NaN, or pd.NA values with count and preview of affected rows. Percentile weight trimming across platforms trim_weights() now computes thresholds via percentile quantiles with explicit clipping bounds for consistent behavior across Python/NumPy versions.Breaking change: percentile-based clipping may shift by roughly one observation at typical limits. IPW diagnostics improvements Fixed multi_class reporting, normalized scalar hyperparameters to floats, removed deprecated penalty argument warnings, and deduplicated metric entries for stable counts across sklearn versions. ","version":"Next","tagName":"h2"},{"title":"Tests​","type":1,"pageTitle":"0.16.0 (Unreleased)","url":"/docs/docs/changelog/#tests-1","content":"Added Windows and macOS CI testing support Expanded GitHub Actions to run on ubuntu-latest, macos-latest, andwindows-latest for Python 3.9-3.14.Added tempfile_path() context manager for cross-platform temp file handling and configured matplotlib Agg backend via conftest.py. ","version":"Next","tagName":"h2"},{"title":"Contributors​","type":1,"pageTitle":"0.16.0 (Unreleased)","url":"/docs/docs/changelog/#contributors-1","content":"@neuralsorcerer, @talgalili, @wesleytlee 0.13.0 (2025-12-02) ","version":"Next","tagName":"h2"},{"title":"New Features​","type":1,"pageTitle":"0.16.0 (Unreleased)","url":"/docs/docs/changelog/#new-features-3","content":"Propensity modeling beyond static logistic regression .adjust(method='ipw') now accepts any sklearn classifier via the modelargument, enabling the use of models like random forests and gradient boosting while preserving all existing trimming and diagnostic features. Dense-only estimators and models without linear coefficients are fully supported. Propensity probabilities are stabilized to avoid numerical issues.Allow customization of logistic regression by passing a configured :class:~sklearn.linear_model.LogisticRegression instance through themodel argument. Also, the CLI now accepts--ipw_logistic_regression_kwargs JSON to build that estimator directly for command-line workflows. Covariate diagnostics Added KL divergence calculations for covariate comparisons (numeric and one-hot categorical), exposed via sample.covars().kld() alongside linked-sample aggregation support. Weighting Methods rake() and poststratify() now honour weight_trimming_mean_ratio andweight_trimming_percentile, trimming and renormalising weights through the enhanced trim_weights(..., target_sum_weights=...) API so the documented parameters work as expected (#147). ","version":"Next","tagName":"h2"},{"title":"Documentation​","type":1,"pageTitle":"0.16.0 (Unreleased)","url":"/docs/docs/changelog/#documentation-1","content":"Added comprehensive post-stratification tutorial notebook (balance_quickstart_poststratify.ipynb) (#141,#142,#143).Expanded poststratify docstring with clear examples and improved statistical methods documentation (#141).Added project badges to README for build status, Python version support, and release tracking (#145).Added example of using custom logistic regression and custom sklearn classifier usage in (balance_quickstart.ipynb).Shorten the welcome message (for when importing the package). ","version":"Next","tagName":"h2"},{"title":"Code Quality & Refactoring​","type":1,"pageTitle":"0.16.0 (Unreleased)","url":"/docs/docs/changelog/#code-quality--refactoring-2","content":"Raking algorithm refactor Removed ipfn dependency and replaced with a vectorized NumPy implementation (_run_ipf_numpy) for iterative proportional fitting, resulting in significant performance improvements and eliminating external dependency (#135). IPW method refactoring Reduced Cyclomatic Complexity Number (CCN) by extracting repeated code patterns into reusable helper functions: _compute_deviance(),_compute_proportion_deviance(), _convert_to_dense_array().Removed manual ASMD improvement calculation and now uses existingcompute_asmd_improvement() from weighted_comparisons_stats.py Type safety improvements Migrated 32 Python files from # pyre-unsafe to # pyre-strict mode, covering core modules, statistics, weighting methods, datasets, and test filesModernized type hints to PEP 604 syntax (X | Y instead of Union[X, Y]) across 11 files for improved readability and Python 3.10+ alignmentType alias definitions in typing.py retain Union syntax for Python 3.9 compatibilityEnhanced plotting function type safety with TypedDict definitions and proper type narrowingReplaced assert-based type narrowing with _verify_value_type() helper for better error messages and pyre-strict compliance Renamed BalanceDF to BalanceDF**** BalanceCovarsDF to BalanceDFCovarsBalanceOutcomesDF to BalanceDFOutcomesBalanceWeightsDF to BalanceDFWeights ","version":"Next","tagName":"h2"},{"title":"Bug Fixes​","type":1,"pageTitle":"0.16.0 (Unreleased)","url":"/docs/docs/changelog/#bug-fixes-3","content":"Utility Functions Fixed quantize() to preserve column ordering and use proper TypeError exceptions (#133) Statistical Functions Fixed division by zero in asmd_improvement() when asmd_mean_before is zero, now returns 0.0 for 0% improvement CLI &amp; Infrastructure Replaced deprecated argparse FileType with pathlib.Path (#134) Weight Trimming Fixed trim_weights() to consistently return pd.Series withdtype=np.float64 and preserve original index across both trimming methodsFixed percentile-based winsorization edge case: _validate_limit() now automatically adjusts limits to prevent floating-point precision issues (#144)Enhanced documentation for trim_weights() and _validate_limit() with clearer examples and explanations ","version":"Next","tagName":"h2"},{"title":"Tests​","type":1,"pageTitle":"0.16.0 (Unreleased)","url":"/docs/docs/changelog/#tests-2","content":"Enhanced test coverage for weight trimming withtest_trim_weights_return_type_consistency and 11 comprehensive tests for_validate_limit() covering edge cases, error conditions, and boundary conditions ","version":"Next","tagName":"h2"},{"title":"Contributors​","type":1,"pageTitle":"0.16.0 (Unreleased)","url":"/docs/docs/changelog/#contributors-2","content":"@neuralsorcerer, @talgalili, @wesleytlee 0.12.1 (2025-11-03) ","version":"Next","tagName":"h2"},{"title":"New Features​","type":1,"pageTitle":"0.16.0 (Unreleased)","url":"/docs/docs/changelog/#new-features-4","content":"Added a welcome message when importing the package. ","version":"Next","tagName":"h2"},{"title":"Documentation​","type":1,"pageTitle":"0.16.0 (Unreleased)","url":"/docs/docs/changelog/#documentation-2","content":"Added 'CHANGELOG' to the docs website.https://import-balance.org/docs/docs/CHANGELOG/ ","version":"Next","tagName":"h2"},{"title":"Bug Fixes​","type":1,"pageTitle":"0.16.0 (Unreleased)","url":"/docs/docs/changelog/#bug-fixes-4","content":"Fixed plotly figures in all the tutorials.https://import-balance.org/docs/tutorials/ 0.12.0 (2025-10-14) ","version":"Next","tagName":"h2"},{"title":"New Features​","type":1,"pageTitle":"0.16.0 (Unreleased)","url":"/docs/docs/changelog/#new-features-5","content":"Support for Python 3.13 + 3.14 Update setup.py and CI/CD integration to include Python 3.13 and 3.14.Remove upper version constraints from numpy, pandas, scipy, and scikit-learn dependencies for Python 3.12+. ","version":"Next","tagName":"h2"},{"title":"Contributors​","type":1,"pageTitle":"0.16.0 (Unreleased)","url":"/docs/docs/changelog/#contributors-3","content":"@talgalili, @wesleytlee 0.11.0 (2025-09-24) ","version":"Next","tagName":"h2"},{"title":"New Features​","type":1,"pageTitle":"0.16.0 (Unreleased)","url":"/docs/docs/changelog/#new-features-6","content":"Python 3.12 support - Complete support for Python 3.12 alongside existing Python 3.9, 3.10, and 3.11 support (with CI/CD integration). Implemented Python version-specific dependency constraints - Added conditional version ranges for numpy, pandas, scipy, and scikit-learn that vary based on Python version (e.g., numpy&gt;=1.21.0,&lt;2.0 for Python &lt;3.12, numpy&gt;=1.24.0,&lt;2.1 for Python &gt;=3.12)Pandas compatibility improvements - Replacedvalue_counts(dropna=False) with groupby().size() in frequency table creation to avoid FutureWarningFixed various pandas deprecation warnings and improved DataFrame handling Improved raking algorithm - Completely refactored rake weighting from DataFrame-based to array-based ipfn algorithm using multi-dimensional arrays and itertools for better performance and compatibility with latest Python versions. Variables are now automatically alphabetized to ensure consistent results regardless of input order.poststratify method enhancement - New strict_matching parameter (default True) handles cases where sample cells are not present in target data. When False, issues warning and assigns weight 0 to uncovered samples ","version":"Next","tagName":"h2"},{"title":"Bug Fixes​","type":1,"pageTitle":"0.16.0 (Unreleased)","url":"/docs/docs/changelog/#bug-fixes-5","content":"Type annotations - Enhanced Pyre type hints throughout the codebase, particularly in utility functionsSample class improvements - Fixed weight type assignment (ensuring float64 type), improved DataFrame manipulation with .infer_objects(copy=False) for pandas compatibility, and enhanced weight setting logicWebsite dependencies - Updated various website dependencies including Docusaurus and related packages ","version":"Next","tagName":"h2"},{"title":"Tests​","type":1,"pageTitle":"0.16.0 (Unreleased)","url":"/docs/docs/changelog/#tests-3","content":"Comprehensive test refactoring, including: Enhanced test validation - Added detailed explanations of test methodologies and expected behaviors in docstringsImproved test coverage - Tests now include edge cases like NaN handling, different data types, and error conditionsImproved test organization (more granular) across all test modules (test_stats_and_plots.py, test_balancedf.py, test_ipw.py, test_rake.py, test_cli.py, test_weighted_comparisons_plots.py, test_cbps.py, test_testutil.py, test_adjustment.py, test_util.py, test_sample.py)Updated GitHub workflows to include Python 3.12 in build and test matrixFix 261 &quot;pandas deprecation&quot; warnings!Added type annotations - Converted test_balancedf.py to pyre-strict with. ","version":"Next","tagName":"h2"},{"title":"Documentation​","type":1,"pageTitle":"0.16.0 (Unreleased)","url":"/docs/docs/changelog/#documentation-3","content":"GitHub issue template for support questions - Added structured template to help users ask questions about using the balance package ","version":"Next","tagName":"h2"},{"title":"Contributors​","type":1,"pageTitle":"0.16.0 (Unreleased)","url":"/docs/docs/changelog/#contributors-4","content":"@talgalili, @wesleytlee, @dependabot 0.10.0 (2025-01-06) ","version":"Next","tagName":"h2"},{"title":"New Features​","type":1,"pageTitle":"0.16.0 (Unreleased)","url":"/docs/docs/changelog/#new-features-7","content":"Dependency on glmnet has been removed, and the ipw method now uses sklearn.The transition to sklearn should enable support for newer python versions (3.11) as well as the Windows OS!ipw method uses logistic regression with L2-penalties instead ofL1-penalties for computational reasons. The transition from glmnet to sklearn and use of L2-penalties will lead to slightly different generated weights compared to previous versions of Balance.Unfortunately, the sklearn-based ipw method is generally slower than the previous version by 2-5x. Consider using the new arguments lambda_min,lambda_max, and num_lambdas for a more efficient search over the ipwpenalization space. ","version":"Next","tagName":"h2"},{"title":"Misc​","type":1,"pageTitle":"0.16.0 (Unreleased)","url":"/docs/docs/changelog/#misc","content":"Update license from GPL v2 toMIT license.Updated Python and package compatibility. Balance is now compatible with Python 3.11, but no longer compatible with Python 3.8 due to typing errors. Balance is currently incompatible with Python 3.12 due to the removal of distutils. ","version":"Next","tagName":"h2"},{"title":"Contributors​","type":1,"pageTitle":"0.16.0 (Unreleased)","url":"/docs/docs/changelog/#contributors-5","content":"@wesleytlee, @talgalili, @SarigT 0.9.1 (2023-07-30) ","version":"Next","tagName":"h2"},{"title":"Bug Fixes​","type":1,"pageTitle":"0.16.0 (Unreleased)","url":"/docs/docs/changelog/#bug-fixes-6","content":"Fix E721 flake8 issue (see:https://github.com/facebookresearch/balance/actions/runs/5704381365/job/15457952704)Remove support for python 3.11 from release.yml ","version":"Next","tagName":"h2"},{"title":"Documentation​","type":1,"pageTitle":"0.16.0 (Unreleased)","url":"/docs/docs/changelog/#documentation-4","content":"Added links to presentation given at ISA 2023. Fixed misc typos. 0.9.0 (2023-05-22) ","version":"Next","tagName":"h2"},{"title":"News​","type":1,"pageTitle":"0.16.0 (Unreleased)","url":"/docs/docs/changelog/#news","content":"Remove support for python 3.11 due to new test failures. This will be the case until glmnet will be replaced by sklearn. hopefully before end of year. ","version":"Next","tagName":"h2"},{"title":"New Features​","type":1,"pageTitle":"0.16.0 (Unreleased)","url":"/docs/docs/changelog/#new-features-8","content":"All plotly functions: add kwargs to pass arguments to update_layout in all plotly figures. This is useful to control width and height of the plot. For example, when wanting to save a high resolution of the image.Add a summary methods to BalanceWeightsDF (i.e.:Sample.weights().summary()) to easily get access to summary statistics of the survey weights. Also, it means that Sample.diagnostics() now uses this new summary method in its internal implementation.BalanceWeightsDF.plot method now relies on the default BalanceDF.plotmethod. This means that instead of a static seaborn kde plot we'll get an interactive plotly version. ","version":"Next","tagName":"h2"},{"title":"Bug Fixes​","type":1,"pageTitle":"0.16.0 (Unreleased)","url":"/docs/docs/changelog/#bug-fixes-7","content":"datasets Remove a no-op in load_data and accommodate deprecation of pandas syntax by using a list rather than a set when selecting df columns (thanks @ahakso for the PR).Make the outcome variable (happiness) be properly displayed in the tutorials (so we can see the benefit of the weighting process). This included fixing the simulation code in the target. Fix Sample.outcomes().summary() so it will output the ci columns without truncating them. ","version":"Next","tagName":"h2"},{"title":"Documentation​","type":1,"pageTitle":"0.16.0 (Unreleased)","url":"/docs/docs/changelog/#documentation-5","content":"Fix text based on updated from version 0.7.0 and 0.8.0. https://import-balance.org/docs/docs/general_framework/adjusting_sample_to_population/ Fix tutorials to include the outcome in the target. ","version":"Next","tagName":"h2"},{"title":"Contributors​","type":1,"pageTitle":"0.16.0 (Unreleased)","url":"/docs/docs/changelog/#contributors-6","content":"@talgalili, @SarigT, @ahakso 0.8.0 (2023-04-26) ","version":"Next","tagName":"h2"},{"title":"New Features​","type":1,"pageTitle":"0.16.0 (Unreleased)","url":"/docs/docs/changelog/#new-features-9","content":"Add rake method to .adjust (currently in beta, given that it doesn't handles marginal target as input).Add a new function prepare_marginal_dist_for_raking - to take in a dict of marginal proportions and turn them into a pandas DataFrame. This can serve as an input target population for raking. ","version":"Next","tagName":"h2"},{"title":"Misc​","type":1,"pageTitle":"0.16.0 (Unreleased)","url":"/docs/docs/changelog/#misc-1","content":"The ipw function now gets max_de=None as default (instead of 1.5). This version is faster, and the user can still choose a threshold as desired.Adding hex stickers graphics files ","version":"Next","tagName":"h2"},{"title":"Documentation​","type":1,"pageTitle":"0.16.0 (Unreleased)","url":"/docs/docs/changelog/#documentation-6","content":"New section onraking.New notebook (in the tutorial section): quickstart_rake - like thequickstarttutorial, but shows how to use the rake (raking) algorithm and compares the results to IPW (logistic regression with LASSO). ","version":"Next","tagName":"h2"},{"title":"Contributors​","type":1,"pageTitle":"0.16.0 (Unreleased)","url":"/docs/docs/changelog/#contributors-7","content":"@talgalili, @SarigT 0.7.0 (2023-04-10) ","version":"Next","tagName":"h2"},{"title":"New Features​","type":1,"pageTitle":"0.16.0 (Unreleased)","url":"/docs/docs/changelog/#new-features-10","content":"Add plotly_plot_density function: Plots interactive density plots of the given variables using kernel density estimation.Modified plotly_plot_dist and plot_dist to also support 'kde' plots. Also, these are now the default options. This automatically percolates toBalanceDF.plot() methods.Sample.from_frame can now guess that a column called &quot;weights&quot; is a weight column (instead of only guessing so if the column is called &quot;weight&quot;). ","version":"Next","tagName":"h2"},{"title":"Bug Fixes​","type":1,"pageTitle":"0.16.0 (Unreleased)","url":"/docs/docs/changelog/#bug-fixes-8","content":"Fix rm_mutual_nas: it now remembers the index of pandas.Series that were used as input. This fixed erroneous plots produced by seaborn functions which uses rm_mutual_nas.Fix plot_hist_kde to work when dist_type = &quot;ecdf&quot;Fix plot_hist_kde and plot_bar when having an input only with &quot;self&quot; and &quot;target&quot;, by fixing _return_sample_palette. ","version":"Next","tagName":"h2"},{"title":"Misc​","type":1,"pageTitle":"0.16.0 (Unreleased)","url":"/docs/docs/changelog/#misc-2","content":"All plotting functions moved internally to expect weight column to be calledweight, instead of weights.All adjust (ipw, cbps, poststratify, null) functions now export a dict with a key called weight instead of weights. ","version":"Next","tagName":"h2"},{"title":"Contributors​","type":1,"pageTitle":"0.16.0 (Unreleased)","url":"/docs/docs/changelog/#contributors-8","content":"@talgalili, @SarigT 0.6.0 (2023-04-05) ","version":"Next","tagName":"h2"},{"title":"New Features​","type":1,"pageTitle":"0.16.0 (Unreleased)","url":"/docs/docs/changelog/#new-features-11","content":"Variance of the weighted mean Add the var_of_weighted_mean function (from balance.stats_and_plots.weighted_stats import var_of_weighted_mean): Computes the variance of the weighted average (pi estimator for ratio-mean) of a list of values and their corresponding weights. Added the var_of_mean option to stat in the descriptive_stats function (based on var_of_weighted_mean)Added the .var_of_mean() method to BalanceDF. Add the ci_of_weighted_mean function (from balance.stats_and_plots.weighted_stats import ci_of_weighted_mean): Computes the confidence intervals of the weighted mean using the (just added) variance of the weighted mean. Added the ci_of_mean option to stat in the descriptive_stats function (based on ci_of_weighted_mean). Also added kwargs support.Added the .ci_of_mean() method to BalanceDF.Added the .mean_with_ci() method to BalanceDF.Updated .summary() methods to include the output of ci_of_mean. All bar plots now have an added ylim argument to control the limits of the y axis. For example use:plot_dist(dfs1, names=[&quot;self&quot;, &quot;unadjusted&quot;, &quot;target&quot;], ylim = (0,1)) Or this: s3_null.covars().plot(ylim = (0,1))Improve 'choose_variables' function to control the order of the returned variables The return type is now a list (and not a Tuple)The order of the returned list is based on the variables argument. If it is not supplied, it is based on the order of the column names in the DataFrames. The df_for_var_order arg controls which df to use. Misc The _prepare_input_model_matrix and downstream functions (e.g.:model_matrix, sample.outcomes().mean(), etc) can now handle DataFrame with special characters in the column names, by replacing special characters with '_' (or '_i', if we end up with columns with duplicate names). It also handles cases in which the column names have duplicates (using the new_make_df_column_names_unique function).Improve choose_variables to control the order of the returned variables The return type is now a list (and not a Tuple)The order of the returned list is based on the variables argument. If it is not supplied, it is based on column names in the DataFrames. The df_for_var_order arg controls which df to use. ","version":"Next","tagName":"h2"},{"title":"Contributors​","type":1,"pageTitle":"0.16.0 (Unreleased)","url":"/docs/docs/changelog/#contributors-9","content":"@talgalili, @SarigT 0.5.0 (2023-03-06) ","version":"Next","tagName":"h2"},{"title":"New Features​","type":1,"pageTitle":"0.16.0 (Unreleased)","url":"/docs/docs/changelog/#new-features-12","content":"The datasets.load_data function now also supports the input &quot;sim_data_cbps&quot;, which loads the simulated data used in the CBPS R vs Python tutorial. It is also used in unit-testing to compare the CBPS weights produced from Python (i.e.: balance) with R (i.e.: the CBPS package). The testing shows how the correlation of the weights from the two implementations (both Pearson and Spearman) produce a correlation of &gt;0.98.cli improvements: Add an option to set formula (as string) in the cli. ","version":"Next","tagName":"h2"},{"title":"Documentation​","type":1,"pageTitle":"0.16.0 (Unreleased)","url":"/docs/docs/changelog/#documentation-7","content":"New notebook (in the tutorial section): Comparing results of fitting CBPS between R's CBPS package and Python'sbalance package (using simulated data).link ","version":"Next","tagName":"h2"},{"title":"Contributors​","type":1,"pageTitle":"0.16.0 (Unreleased)","url":"/docs/docs/changelog/#contributors-10","content":"@stevemandala, @talgalili, @SarigT 0.4.0 (2023-02-08) ","version":"Next","tagName":"h2"},{"title":"New Features​","type":1,"pageTitle":"0.16.0 (Unreleased)","url":"/docs/docs/changelog/#new-features-13","content":"Added two new flags to the cli: --standardize_types: This gives cli users the ability to set thestandardize_types parameter in Sample.from_frame to True or False. To learn more about this parameter, see:https://import-balance.org/api_reference/html/balance.sample_class.html#balance.sample_class.Sample.from_frame--return_df_with_original_dtypes: the Sample object now stores the dtypes of the original df that was read using Sample.from_frame. This can be used to restore the original dtypes of the file output from the cli. This is relevant in cases in which we want to convert back the dtypes of columns from how they are stored in Sample, to their original types (e.g.: if something was Int32 it would be turned in float32 in balance.Sample, and using the new flag will return that column, when using the cli, to be back in the Int32 type). This feature may not be robust to various edge cases. So use with caution. In the logging: Added warnings about dtypes changes. E.g.: if using Sample.from_frame with a column that has Int32, it will be turned into float32 in the internal storage of sample. Now there will be a warning message indicating of this change.Increase the default length of logger printing (from 500 to 2000) ","version":"Next","tagName":"h2"},{"title":"Bug Fixes​","type":1,"pageTitle":"0.16.0 (Unreleased)","url":"/docs/docs/changelog/#bug-fixes-9","content":"Fix pandas warning: SettingWithCopyWarning in from_frame (and other places in sample_class.py)sample.from_frame has a new argument use_deepcopy to decide if changes made to the df inside the sample object would also change the original df that was provided to the sample object. The default is now set to True since it's more likely that we'd like to keep the changes inside the sample object to the df contained in it, and not have them spill into the original df. ","version":"Next","tagName":"h2"},{"title":"Contributors​","type":1,"pageTitle":"0.16.0 (Unreleased)","url":"/docs/docs/changelog/#contributors-11","content":"@SarigT, @talgalili 0.3.1 (2023-02-01) ","version":"Next","tagName":"h2"},{"title":"Bug Fixes​","type":1,"pageTitle":"0.16.0 (Unreleased)","url":"/docs/docs/changelog/#bug-fixes-10","content":"Sample.from_frame now also converts int16 and in8 to float16 and float16. Thus helping to avoid TypeError: Cannot interpret 'Int16Dtype()' as a data typestyle errors. ","version":"Next","tagName":"h2"},{"title":"Documentation​","type":1,"pageTitle":"0.16.0 (Unreleased)","url":"/docs/docs/changelog/#documentation-8","content":"Added ISSUE_TEMPLATE ","version":"Next","tagName":"h2"},{"title":"Contributors​","type":1,"pageTitle":"0.16.0 (Unreleased)","url":"/docs/docs/changelog/#contributors-12","content":"@talgalili, @stevemandala, @SarigT 0.3.0 (2023-01-30) ","version":"Next","tagName":"h2"},{"title":"New Features​","type":1,"pageTitle":"0.16.0 (Unreleased)","url":"/docs/docs/changelog/#new-features-14","content":"Added compatibility for Python 3.11 (by supporting SciPy 1.9.2) (props to @tomwagstaff-opml for flagging this issue).Added the session-info package as a dependency. ","version":"Next","tagName":"h2"},{"title":"Bug Fixes​","type":1,"pageTitle":"0.16.0 (Unreleased)","url":"/docs/docs/changelog/#bug-fixes-11","content":"Fixed pip install from source on Windows machines (props to @tomwagstaff-opml for the bug report). ","version":"Next","tagName":"h2"},{"title":"Documentation​","type":1,"pageTitle":"0.16.0 (Unreleased)","url":"/docs/docs/changelog/#documentation-9","content":"Added session_info.show() outputs to the end of the three tutorials (at:https://import-balance.org/docs/tutorials/)Misc updates to the README. ","version":"Next","tagName":"h2"},{"title":"Contributors​","type":1,"pageTitle":"0.16.0 (Unreleased)","url":"/docs/docs/changelog/#contributors-13","content":"@stevemandala, @SarigT, @talgalili 0.2.0 (2023-01-19) ","version":"Next","tagName":"h2"},{"title":"New Features​","type":1,"pageTitle":"0.16.0 (Unreleased)","url":"/docs/docs/changelog/#new-features-15","content":"cli improvements: Add an option to set weight_trimming_mean_ratio = None for no trimming.Add an option to set transformations to be None (i.e. no transformations). Add an option to adapt the title in: stats_and_plots.weighted_comparison_plots.plot_barstats_and_plots.weighted_comparison_plots.plot_hist_kde ","version":"Next","tagName":"h2"},{"title":"Bug Fixes​","type":1,"pageTitle":"0.16.0 (Unreleased)","url":"/docs/docs/changelog/#bug-fixes-12","content":"Fix (and simplify) balanceDF.plot to organize the order of groups (now unadjusted/self is left, adjusted/self center, and target is on the right)Fix plotly functions to use the red color for self when only compared to target (since in that case it is likely unadjusted): balance.stats_and_plots.weighted_comparisons_plots.plotly_plot_qq and balance.stats_and_plots.weighted_comparisons_plots.plotly_plot_barFix seaborn_plot_dist: output None by default (instead of axis object). Added a return_Axes argument to control this behavior.Fix some test_cbps tests that were failing due to non-exact matches (we made the test less sensitive) ","version":"Next","tagName":"h2"},{"title":"Documentation​","type":1,"pageTitle":"0.16.0 (Unreleased)","url":"/docs/docs/changelog/#documentation-10","content":"New blog section, with the post:Bringing &quot;balance&quot; to your data New tutorial: quickstart_cbps - like thequickstarttutorial, but shows how to use the CBPS algorithm and compares the results to IPW (logistic regression with LASSO).balance_transformations_and_formulas - This tutorial showcases ways in which transformations, formulas and penalty can be included in your pre-processing of the covariates before adjusting for them. API docs: New: highlighting on codeblocksa bunch of text fixes. Update README.md logowith contributorstypo fixes (props to @zbraiterman and @luca-martial). Added section about &quot;Releasing a new version&quot; to CONTRIBUTING.md Available under&quot;Docs/Contributing&quot;section of website ","version":"Next","tagName":"h2"},{"title":"Misc​","type":1,"pageTitle":"0.16.0 (Unreleased)","url":"/docs/docs/changelog/#misc-3","content":"Added automated Github Action package builds &amp; deployment to PyPi on release. Seerelease.yml ","version":"Next","tagName":"h2"},{"title":"Contributors​","type":1,"pageTitle":"0.16.0 (Unreleased)","url":"/docs/docs/changelog/#contributors-14","content":"@stevemandala, @SarigT, @talgalili 0.1.0 (2022-11-20) ","version":"Next","tagName":"h2"},{"title":"Summary​","type":1,"pageTitle":"0.16.0 (Unreleased)","url":"/docs/docs/changelog/#summary","content":"balance released to the world! ","version":"Next","tagName":"h2"},{"title":"Contributors​","type":1,"pageTitle":"0.16.0 (Unreleased)","url":"/docs/docs/changelog/#contributors-15","content":"@SarigT, @talgalili, @stevemandala ","version":"Next","tagName":"h2"},{"title":"balance: a python package for balancing biased data samples","type":0,"sectionRef":"#","url":"/docs/docs/overview/","content":"","keywords":"","version":"Next"},{"title":"What is balance?​","type":1,"pageTitle":"balance: a python package for balancing biased data samples","url":"/docs/docs/overview/#what-is-balance","content":"balance is a Python package offering a simple workflow and methods for dealing with biased data samples when looking to infer from them to some population of interest. Biased samples often occur insurvey statistics when respondents presentnon-response bias or survey suffers from sampling bias (that are notmissing completely at random). A similar issue arises inobservational studies when comparing the treated vs untreated groups, and in any data that suffers from selection bias. Under the missing at random assumption (MAR), bias in samples could sometimes be (at least partially) mitigated by relying on auxiliary information (a.k.a.: &quot;covariates&quot; or &quot;features&quot;) that is present for all items in the sample, as well as present in a sample of items from the population. For example, if we want to infer from a sample of respondents to some survey, we may wish to adjust for non-response using demographic information such as age, gender, education, etc. This can be done by weighing the sample to the population using auxiliary information. The package is intended for researchers who are interested in balancing biased samples, such as the ones coming from surveys, using a Python package. This need may arise by survey methodologists, demographers, UX researchers, market researchers, and generally data scientists, statisticians, and machine learners. More about the methodological background can be found inSarig, T., Galili, T., &amp; Eilat, R. (2023). balance – a Python package for balancing biased data samples. Installation ","version":"Next","tagName":"h2"},{"title":"Requirements​","type":1,"pageTitle":"balance: a python package for balancing biased data samples","url":"/docs/docs/overview/#requirements","content":"You need Python 3.9, 3.10, 3.11, 3.12, 3.13, or 3.14 to run balance. balancecan be built and run from Linux, OSX, and Windows. The required Python dependencies are: REQUIRES = [ # Numpy and pandas: carefully versioned for binary compatibility &quot;numpy&gt;=1.21.0,&lt;2.0; python_version&lt;'3.12'&quot;, &quot;numpy&gt;=1.24.0; python_version&gt;='3.12'&quot;, &quot;pandas&gt;=1.5.0,&lt;2.4.0; python_version&lt;'3.12'&quot;, &quot;pandas&gt;=2.0.0; python_version&gt;='3.12'&quot;, # Scientific stack &quot;scipy&gt;=1.7.0,&lt;1.14.0; python_version&lt;'3.12'&quot;, &quot;scipy&gt;=1.11.0; python_version&gt;='3.12'&quot;, &quot;scikit-learn&gt;=1.0.0,&lt;1.4.0; python_version&lt;'3.12'&quot;, &quot;scikit-learn&gt;=1.3.0; python_version&gt;='3.12'&quot;, &quot;ipython&quot;, &quot;patsy&quot;, &quot;seaborn&quot;, &quot;plotly&quot;, &quot;matplotlib&quot;, &quot;statsmodels&quot;, &quot;session-info&quot;, ]  Seepyproject.tomlfor more details. ","version":"Next","tagName":"h2"},{"title":"Installing balance​","type":1,"pageTitle":"balance: a python package for balancing biased data samples","url":"/docs/docs/overview/#installing-balance","content":"","version":"Next","tagName":"h2"},{"title":"Installing via PyPi​","type":1,"pageTitle":"balance: a python package for balancing biased data samples","url":"/docs/docs/overview/#installing-via-pypi","content":"We recommend installing balance from PyPi via pip for the latest stable version: python -m pip install balance  Installation will use Python wheels from PyPI, available forOSX, Linux, and Windows. ","version":"Next","tagName":"h3"},{"title":"Installing from Source/Git​","type":1,"pageTitle":"balance: a python package for balancing biased data samples","url":"/docs/docs/overview/#installing-from-sourcegit","content":"You can install the latest (bleeding edge) version from Git: python -m pip install git+https://github.com/facebookresearch/balance.git  Alternatively, if you have a local clone of the repo: cd balance python -m pip install .  Or using dev-dependencies: cd balance python -m pip install .[dev]  Getting started ","version":"Next","tagName":"h3"},{"title":"balance's workflow in high-level​","type":1,"pageTitle":"balance: a python package for balancing biased data samples","url":"/docs/docs/overview/#balances-workflow-in-high-level","content":"The core workflow in balance deals with fitting and evaluating weights to a sample. For each unit in the sample (such as a respondent to a survey), balance fits a weight that can be (loosely) interpreted as the number of people from the target population that this respondent represents. This aims to help mitigate the coverage and non-response biases, as illustrated in the following figure.  The weighting of survey data through balance is done in the following main steps: Loading data of the respondents of the survey.Loading data about the target population we would like to correct for.Diagnostics of the sample covariates so to evaluate whether weighting is needed.Adjusting the sample to the target.Evaluation of the results.Use the weights for producing population level estimations.Saving the output weights. You can see a step-by-step description (with code) of the above steps in theGeneral Frameworkpage. ","version":"Next","tagName":"h2"},{"title":"Code example of using balance​","type":1,"pageTitle":"balance: a python package for balancing biased data samples","url":"/docs/docs/overview/#code-example-of-using-balance","content":"You may run the following code to play with balance's basic workflow (these are snippets taken from thequickstart tutorial): We start by loading data, and adjusting it: from balance import load_data, Sample # load simulated example data target_df, sample_df = load_data() # Import sample and target data into a Sample object sample = Sample.from_frame(sample_df, outcome_columns=[&quot;happiness&quot;]) target = Sample.from_frame(target_df) # Set the target to be the target of sample sample_with_target = sample.set_target(target) # Check basic diagnostics of sample vs target before adjusting: # sample_with_target.covars().plot()  You can read more on evaluation of the pre-adjusted data in thePre-Adjustment Diagnosticspage. Next, we adjust the sample to the population by fitting balancing survey weights: # Using ipw to fit survey weights adjusted = sample_with_target.adjust()  You can read more on adjustment process in theAdjusting Sample to Populationpage. The above code gets us an adjusted object with weights. We can evaluate the benefit of the weights to the covariate balance, for example by running: print(adjusted.summary()) # Covar ASMD reduction: 62.3%, design effect: 2.249 # Covar ASMD (7 variables):0.335 -&gt; 0.126 # Model performance: Model proportion deviance explained: 0.174 adjusted.covars().plot(library = &quot;seaborn&quot;, dist_type = &quot;kde&quot;)  And get:  We can also check the impact of the weights on the outcome using: # For the outcome: print(adjusted.outcomes().summary()) # 1 outcomes: ['happiness'] # Mean outcomes: # happiness # source # self 54.221388 # unadjusted 48.392784 # # Response rates (relative to number of respondents in sample): # happiness # n 1000.0 # % 100.0 adjusted.outcomes().plot()   You can read more on evaluation of the post-adjusted data in theEvaluating and using the adjustment weightspage. Finally, the adjusted data can be downloaded using: adjusted.to_download() # Or: # adjusted.to_csv()  To see a more detailed step-by-step code example with code output prints and plots (both static and interactive), please go over to thetutorials section. ","version":"Next","tagName":"h2"},{"title":"Implemented methods for adjustments​","type":1,"pageTitle":"balance: a python package for balancing biased data samples","url":"/docs/docs/overview/#implemented-methods-for-adjustments","content":"balance currently implements various adjustment methods. Click the links to learn more about each: Logistic regression using L1 (LASSO) penalization.Covariate Balancing Propensity Score (CBPS).Post-stratification.Raking. ","version":"Next","tagName":"h2"},{"title":"Implemented methods for diagnostics/evaluation​","type":1,"pageTitle":"balance: a python package for balancing biased data samples","url":"/docs/docs/overview/#implemented-methods-for-diagnosticsevaluation","content":"For diagnostics the main tools (comparing before, after applying weights, and the target population) are: Plots barplotsdensity plots (for weights and covariances)qq-plots Statistical summaries Weights distributions Kish's design effectMain summaries (mean, median, variances, quantiles) Covariate distributions Absolute Standardized Mean Difference (ASMD). For continuous variables, it is Cohen's d. Categorical variables are one-hot encoded, Cohen's d is calculated for each category and ASMD for a categorical variable is defined as Cohen's d, average across all categories. You can read more on evaluation of the post-adjusted data in theEvaluating and using the adjustment weightspage. ","version":"Next","tagName":"h2"},{"title":"Other resources​","type":1,"pageTitle":"balance: a python package for balancing biased data samples","url":"/docs/docs/overview/#other-resources","content":"Presentation:&quot;Balancing biased data samples with the 'balance' Python package&quot; - presented in the Israeli Statistical Association (ISA) conference on June 1st 2023. More details ","version":"Next","tagName":"h2"},{"title":"Getting help, submitting bug reports and contributing code​","type":1,"pageTitle":"balance: a python package for balancing biased data samples","url":"/docs/docs/overview/#getting-help-submitting-bug-reports-and-contributing-code","content":"You are welcome to: Learn more in the balance website.Ask for help on:https://github.com/facebookresearch/balance/issues/new?template=support_question.mdSubmit bug-reports and features' suggestions at:https://github.com/facebookresearch/balance/issuesSend a pull request on: https://github.com/facebookresearch/balance. See theCONTRIBUTINGfile for how to help out. And ourCODE OF CONDUCTfor our expectations from contributors. ","version":"Next","tagName":"h2"},{"title":"Citing balance​","type":1,"pageTitle":"balance: a python package for balancing biased data samples","url":"/docs/docs/overview/#citing-balance","content":"Sarig, T., Galili, T., &amp; Eilat, R. (2023). balance – a Python package for balancing biased data samples.https://arxiv.org/abs/2307.06024 @misc{sarig2023balance, title={balance - a Python package for balancing biased data samples}, author={Tal Sarig and Tal Galili and Roee Eilat}, year={2023}, eprint={2307.06024}, archivePrefix={arXiv}, primaryClass={stat.CO} }  ","version":"Next","tagName":"h2"},{"title":"License​","type":1,"pageTitle":"balance: a python package for balancing biased data samples","url":"/docs/docs/overview/#license","content":"The balance package is licensed under theMIT license, and all the documentation on the site (including text and images) is underCC-BY. News You can follow updates on our: BlogChangelog ","version":"Next","tagName":"h2"},{"title":"Acknowledgements / People​","type":1,"pageTitle":"balance: a python package for balancing biased data samples","url":"/docs/docs/overview/#acknowledgements--people","content":"The balance package is actively maintained by people from theCentral Applied Scienceteam (in Menlo Park and Tel Aviv), byWesley Lee,Tal Sarig, andTal Galili. The balance package was (and is) developed by many people, including:Roee Eilat,Tal Galili,Daniel Haimovich,Kevin Liou,Steve Mandala,Adam Obeng (author of the initial internal Meta version), Tal Sarig,Luke Sonnet,Sean Taylor,Barak Yair Reif,Soumyadip Sarkar, and others. If you worked on balance in the past, please email us to be added to this list. The balance package was open-sourced byTal Sarig,Tal Galili andSteve Mandala in late 2022. Branding created by Dana Beaty, from the Meta AI Design and Marketing Team. For logo files, seehere. ","version":"Next","tagName":"h2"},{"title":"Evaluating and using the adjustment weights","type":0,"sectionRef":"#","url":"/docs/docs/general_framework/evaluation_of_results/","content":"","keywords":"diagnostics evaluation results","version":"Next"},{"title":"Summary statistics​","type":1,"pageTitle":"Evaluating and using the adjustment weights","url":"/docs/docs/general_framework/evaluation_of_results/#summary-statistics","content":"","version":"Next","tagName":"h2"},{"title":"Summary​","type":1,"pageTitle":"Evaluating and using the adjustment weights","url":"/docs/docs/general_framework/evaluation_of_results/#summary","content":"Printing the adjusted object gives a high level overview of the content of the object: print(adjusted)  Output: Adjusted balance Sample object with target set using ipw 1000 observations x 3 variables: gender,age_group,income id_column: id, weight_column: weight, outcome_columns: happiness target: balance Sample object 10000 observations x 3 variables: gender,age_group,income id_column: id, weight_column: weight, outcome_columns: None 3 common variables: income,age_group,gender  To generate a summary of the data, use the summary method: print(adjusted.summary())  This will return several results: Covariate mean ASMD improvement: ASMD is &quot;Absolute Standardized Mean Difference&quot;. For continuous variables, this measure is the same as taking the absolute value of Cohen's d statistic (also related to SSMD), when using the (weighted) standard deviation of the target population. For categorical variables it uses one-hot encoding.Design effectCovariate mean Adjusted Standardized Mean Deviation (ASMD) versus Unadjusted covariate mean ASMDModel proportion deviance explained (if inverese propensity weighting method was used) Output: Covar ASMD reduction: 62.3%, design effect: 2.249 Covar ASMD (7 variables): 0.335 -&gt; 0.126 Model performance: Model proportion deviance explained: 0.174  Note that although we had 3 variables in our original data (age_group, gender, income), the asmd counts each level of the categorical variables as separate variable, and thus it considered 7 variables for the covar ASMD improvement. ","version":"Next","tagName":"h3"},{"title":"Covariate Balance​","type":1,"pageTitle":"Evaluating and using the adjustment weights","url":"/docs/docs/general_framework/evaluation_of_results/#covariate-balance","content":"We can check the mean of each variable before and after applying the weights using .mean(): adjusted.covars().mean().T  To get: source self target unadjusted _is_na_gender[T.True] 0.103449 0.089800 0.08800 age_group[T.25-34] 0.279072 0.297400 0.30900 age_group[T.35-44] 0.290137 0.299200 0.17200 age_group[T.45+] 0.150714 0.206300 0.04600 gender[Female] 0.410664 0.455100 0.26800 gender[Male] 0.485887 0.455100 0.64400 gender[_NA] 0.103449 0.089800 0.08800 income 9.519935 12.737608 5.99102  The self is the adjusted ASMD, while unadjusted is the unadjusted ASMD. And .asmd() to get ASMD: adjusted.covars().asmd().T  To get: source self unadjusted unadjusted - self age_group[T.25-34] 0.040094 0.025375 -0.014719 age_group[T.35-44] 0.019792 0.277771 0.257980 age_group[T.45+] 0.137361 0.396127 0.258765 gender[Female] 0.089228 0.375699 0.286472 gender[Male] 0.061820 0.379314 0.317494 gender[_NA] 0.047739 0.006296 -0.041444 income 0.246918 0.517721 0.270802 mean(asmd) 0.126310 0.334860 0.208551  We can see that on average the ASMD improved from 0.33 to 0.12 thanks to the weights. We got improvements in income, gender, and age_group. Although we can see that age_group[T.25-34] didn't get improved. ","version":"Next","tagName":"h2"},{"title":"Understanding the model​","type":1,"pageTitle":"Evaluating and using the adjustment weights","url":"/docs/docs/general_framework/evaluation_of_results/#understanding-the-model","content":"For a summary of the diagnostics measures, use: adjusted.diagnostics()  This will give a long table that can be filterred to focus on various diagnostics metrics. For example, when the .adjust() method is run with model=&quot;ipw&quot; (the default method), then the rows from the diagnostics output with metric == &quot;model_coef&quot; represent the coefficients of the variables in the model. These can be used to understand the model that was fitted (after transformations and regularization). ","version":"Next","tagName":"h2"},{"title":"Visualization post adjustments​","type":1,"pageTitle":"Evaluating and using the adjustment weights","url":"/docs/docs/general_framework/evaluation_of_results/#visualization-post-adjustments","content":"We can create all (interactive) plots using: adjusted.covars().plot()  And get:    We can also use different plots, using the seaborn library, for example with the &quot;kde&quot; dist_type. adjusted.covars().plot(library = &quot;seaborn&quot;, dist_type = &quot;kde&quot;)  And get:  ","version":"Next","tagName":"h2"},{"title":"Distribution of Weights​","type":1,"pageTitle":"Evaluating and using the adjustment weights","url":"/docs/docs/general_framework/evaluation_of_results/#distribution-of-weights","content":"We can look at the distribution of weights using the following method call: adjusted.weights().plot()  And get:  Or calculate the design effect using: adjusted.weights().design_effect() # 2.24937  ","version":"Next","tagName":"h2"},{"title":"Analyzing the outcome​","type":1,"pageTitle":"Evaluating and using the adjustment weights","url":"/docs/docs/general_framework/evaluation_of_results/#analyzing-the-outcome","content":"The .summary() method gives us the response rates (if we have missing values in the outcome), and the weighted means before and after applying the weights: print(adjust.outcomes().summary())  To get:  1 outcomes: ['happiness'] Mean outcomes: happiness source self 54.221388 unadjusted 48.392784 Response rates (relative to number of respondents in sample): happiness n 1000.0 % 100.0  For example, we see that the estimated mean happiness according to our sample is 48 without any adjustment and 54 with adjustment. The following shows the distribution of happinnes before and after applying the weights: adjusted.outcomes().plot()  And we get:  ","version":"Next","tagName":"h2"},{"title":"Pre-Adjustment Diagnostics","type":0,"sectionRef":"#","url":"/docs/docs/general_framework/pre_adjustment_diagnostics/","content":"","keywords":"unadjusted sample diagnostics","version":"Next"},{"title":"Covariate balance​","type":1,"pageTitle":"Pre-Adjustment Diagnostics","url":"/docs/docs/general_framework/pre_adjustment_diagnostics/#covariate-balance","content":"A way to check if adjustments are needed is looking at covariate balance by comparing the distribution of covariates in our sample (the respondents before any adjustment), to the distribution of covariates of the population. The same methods will be later used to evaluate the quality of the adjustment in evaluating the results. There are various methods for comparing covariate balance, either via summary statistics, or through visualizations. The visualizations are implemented either via plotly (offering an interactive interface) or seaborn (leading to a static image). The methods implemented in balance include: Summary statistics MeansASMD (Absolute Standardized Mean Difference) Visualizations Numerical variables QQ-plots (interactive)Kernel density estimation (static)Empirical Cumulative Distribution Function (static)Histogram (static) Categorical variables Barplots (interactive or static)Probability scatter plot (static) ","version":"Next","tagName":"h2"},{"title":"Summary statistics​","type":1,"pageTitle":"Pre-Adjustment Diagnostics","url":"/docs/docs/general_framework/pre_adjustment_diagnostics/#summary-statistics","content":"","version":"Next","tagName":"h2"},{"title":"Means and ASMD (Absolute Standardized Mean Difference)​","type":1,"pageTitle":"Pre-Adjustment Diagnostics","url":"/docs/docs/general_framework/pre_adjustment_diagnostics/#means-and-asmd-absolute-standardized-mean-difference","content":"The mean of the covariates in the sample versus the target is a basic measure to evaluate the distance of the sample from the target population of interest. For categorical variables the means are calculated to each of the one-hot encoding of the categories of the variable. This is basically the proportion of observations in that bucket. It can be calculated simply by running: sample_with_target.covars().mean().T  An example of the output: source self target _is_na_gender[T.True] 0.08800 0.089800 age_group[T.25-34] 0.30900 0.297400 age_group[T.35-44] 0.17200 0.299200 age_group[T.45+] 0.04600 0.206300 gender[Female] 0.26800 0.455100 gender[Male] 0.64400 0.455100 gender[_NA] 0.08800 0.089800 income 5.99102 12.737608  (TODO: the one hot encoding acts a bit differently for different variables - this will be resolved in future releases) The limitation of the mean is that it is not easily comparable between different variables since they may have different variances. The simplest attempt in addressing this issue is using the ASMD. The ASMD (Absolute Standardized Mean Deviation) measures the difference per covariate between the sample and target. It uses weighted average and std for the calculations (e.g.: to take design weights into account). This measure is the same as taking the absolute value of Cohen's d statistic (also related to SSMD), when using the (weighted) standard deviation of the target population. Other options that occur in the literature includes using the standard deviation based on the sample, or some average of the std of the sample and the target. In order to allow this to be compared across different samples and adjustments, we opted to use the std of the target as the default. It can be calculated simply by running: sample_with_target.covars().asmd().T  An example of the output: source self age_group[T.25-34] 0.025375 age_group[T.35-44] 0.277771 age_group[T.45+] 0.396127 gender[Female] 0.375699 gender[Male] 0.379314 gender[_NA] 0.006296 income 0.517721 mean(asmd) 0.334860  For categorical variables the ASMD can be calculated as the average of the ASMD applied to each of the one-hot encoding of the categories of the variable by using the aggregate_by_main_covar argument: sample_with_target.covars().asmd(aggregate_by_main_covar = True).T  The output: source self age_group 0.233091 gender 0.253769 income 0.517721 mean(asmd) 0.334860  An average ASMD is calculated for all covariates. It is a simple average of the ASMD for each covariate. Each ASMD value of categorical variable is used once after aggregated the ASMD from all the dummy variables. ","version":"Next","tagName":"h3"},{"title":"Visualizations​","type":1,"pageTitle":"Pre-Adjustment Diagnostics","url":"/docs/docs/general_framework/pre_adjustment_diagnostics/#visualizations","content":"","version":"Next","tagName":"h2"},{"title":"Q-Q Plot (plotly)​","type":1,"pageTitle":"Pre-Adjustment Diagnostics","url":"/docs/docs/general_framework/pre_adjustment_diagnostics/#q-q-plot-plotly","content":"We provide Q-Q Plots as a visual to compare two distributions to one another. For example, the plot below is a Q-Q plot for the income covariate for the sample against a straight line of the target population:  The closer the line is to the 45-degree-line the better (i.e.: the less bias is observed in the sample as compared to the target population). To make a QQ-plot for a specific variable, simply use the following method (the default uses QQ plot with the plotly engine): sample_with_target.covars().plot(variables = ['income',])  ","version":"Next","tagName":"h3"},{"title":"Barplots​","type":1,"pageTitle":"Pre-Adjustment Diagnostics","url":"/docs/docs/general_framework/pre_adjustment_diagnostics/#barplots","content":"Barplots provides a way to visually compare the sample and target for categorical covariates. Here is an example of the plot for age_group and gender before adjustment:   To make these plots, simply use the following: sample_with_target.covars().plot(variables = ['age_group', 'gender', ])  ","version":"Next","tagName":"h3"},{"title":"Plotting all varibales​","type":1,"pageTitle":"Pre-Adjustment Diagnostics","url":"/docs/docs/general_framework/pre_adjustment_diagnostics/#plotting-all-varibales","content":"If you do not specify a variables list in the plot method, all covariates of you sample object will be plotted: sample_with_target.covars().plot()  ","version":"Next","tagName":"h3"},{"title":"Statistical Methods","type":0,"sectionRef":"#","url":"/docs/docs/statistical_methods/","content":"Statistical Methods This section descirbes the statistical methodologies used in balance for weighting: Inverse propensity score weightingCovariate balancing proponsity scorePost-stratificationRaking","keywords":"","version":"Next"},{"title":"Raking","type":0,"sectionRef":"#","url":"/docs/docs/statistical_methods/rake/","content":"","keywords":"rake raking","version":"Next"},{"title":"Introduction​","type":1,"pageTitle":"Raking","url":"/docs/docs/statistical_methods/rake/#introduction","content":"Raking, also known as iterative proportional fitting, is a statistical technique widely used in survey sampling to adjust weights and enhance the representativeness of the collected data. When a sample is drawn from a population, there might be differences in the distribution of certain variables between the sample and the population. Raking, similar to other methods in the balance package, helps to account for these differences, making the sample's distribution closely resemble that of the population. Raking is an iterative process that involves adjusting the weights of sampled units based on the marginal distributions of certain variables in the population. Typically, we have access to such marginal distributions, but not their combined joint distribution. The variables chosen for raking are usually demographic variables, such as age, gender, education, income, and other socioeconomic variables, which are known to influence survey outcomes. By adjusting the weights of the sampled units, raking helps to correct for potential biases that may arise due to nonresponse, undercoverage, or oversampling of certain groups. ","version":"Next","tagName":"h2"},{"title":"Methodology​","type":1,"pageTitle":"Raking","url":"/docs/docs/statistical_methods/rake/#methodology","content":"Raking essentially applies post-stratification repeatedly over all the covariates. For example, we may have the marginal distribution of age*gender and education. Raking would first adjust weights to match the age*gender distribution and then take these weights as input to adjust for education. It would then adjust again to age*gender and then again to education, and so forth. This process will repeat until either a max_iteration is met, or the weights have converged and no longer seem to change from one iteration to another. Raking is a valuable technique for addressing potential biases and enhancing the representativeness of survey data. By iteratively adjusting the weights of sampled units based on the marginal distribution of key variables, raking ensures that survey estimates are more accurate and reliable. You can see a detailed example of how to perform raking in balance in the tutorial: quickstart_rake. ","version":"Next","tagName":"h2"},{"title":"References​","type":1,"pageTitle":"Raking","url":"/docs/docs/statistical_methods/rake/#references","content":"https://en.wikipedia.org/wiki/Rakinghttps://www.pewresearch.org/methods/2018/01/26/how-different-weighting-methods-work/Practical Considerations in Raking Survey Data (url) ","version":"Next","tagName":"h2"},{"title":"Tutorials and notebooks Overview","type":0,"sectionRef":"#","url":"/docs/tutorials/","content":"","keywords":"","version":"Next"},{"title":"Tutorials list (more tutorials to be added soon):​","type":1,"pageTitle":"Tutorials and notebooks Overview","url":"/docs/tutorials/#tutorials-list-more-tutorials-to-be-added-soon","content":"quickstart - this is based on a simulated data and presents the simple end-to-end workflow of balance package with default arguments. It demonstrates the process from reading the data, through understanding the biases in the sample, producing weights, evaluating the results and producing the population estimations.quickstart_cbps - like the quickstart tutorial, but shows how to use the CBPS algorithm and compares the results to IPW (logistic regression with LASSO).quickstart_rake - like the quickstart tutorial, but shows how to use the rake (raking) algorithm and compares the results to IPW (logistic regression with LASSO).quickstart_poststratify - focuses on post-stratification, highlighting how to match either a single marginal distribution or the full joint distribution of two covariates.balance_transformations_and_formulas - This tutorial showcases ways in which transformations, formulas and penalty can be included in your pre-processing of the covariates before adjusting for them.comparing_cbps_in_r_vs_python_using_sim_data - This notebook compares the results of running CBPS in R and Python. In R using the BCPS package, and in Python using the balance package. The results are almost identical.CLI tutorial - Walks through the balance command-line interface workflow, including building an input dataset, running the CLI, and reviewing diagnostics. ","version":"Next","tagName":"h2"},{"title":"Inverse Propensity Score Weighting","type":0,"sectionRef":"#","url":"/docs/docs/statistical_methods/ipw/","content":"","keywords":"inverse propensity score weighting ipw ipsw","version":"Next"},{"title":"Introduction​","type":1,"pageTitle":"Inverse Propensity Score Weighting","url":"/docs/docs/statistical_methods/ipw/#introduction","content":"The inverse propensity score weighting is a statistical method to adjust a non-random sample to represent a population by weighting the sample units. It assumes two samples: (1) A sample of respondents to a survey (or in a more general framework, a biased panel), will be referred to as &quot;sample&quot;. (2) A sample of a target population, often referred to as &quot;reference sample&quot; or &quot;reference survey&quot; [1], will be referred to as &quot;target&quot;. This sample includes a larger coverage of the population or a better sampling properties in a way that represents the population better. It often includes only a limited number of covariates and doesn't include the outcome variables (the survey responses). In different cases it can be the whole target population (in case it is available), a census data (based on a survey) or an existing survey. ","version":"Next","tagName":"h2"},{"title":"Mathematical model​","type":1,"pageTitle":"Inverse Propensity Score Weighting","url":"/docs/docs/statistical_methods/ipw/#mathematical-model","content":"Let SSS represent the sample of respondents, with nnn units, and TTT represent the target population, with NNN units. We may assume each unit iii in the sample and target have a base weight, which is referred to as a design weight, did_idi​. These are often set to be 1 for the sample (assuming unknown sampling probabilities), and are based on the sampling procedure for the target. In addition, we assume all units in sample and target have a covariates vector attached, xix_ixi​. Note that we assume that the same covariates are available for the sample and the target, otherwise we ignore the non-overlapping covariates. Define the propensity score as the probability to be included in the sample (the respondents group) conditioned on the characteristics of the unit, i.e. let pi=Pr{i∈S∣xi}p_i = Pr\\{i \\in S | x_i\\}pi​=Pr{i∈S∣xi​}, i=1...ni=1...ni=1...n. pip_ipi​ is then estimated using logistic regression, assuming a linear relation between the covariates and the logit of the probability: ln⁡(pi1−pi)=β0+β1xi\\ln(\\frac{p_i}{1-p_i})=\\beta_0+\\beta_1 x_iln(1−pi​pi​​)=β0​+β1​xi​. Note that balance's implementation for ipw uses a regularized logistic model through using LASSO (by using glmnet-python). This is in order to keep the inflation of the variance as minimal as possible while still addressing the meaningful differences in the covariates between the sample and the target. ","version":"Next","tagName":"h2"},{"title":"How are the regularization parameter and trimming ratio parameter chosen?​","type":1,"pageTitle":"Inverse Propensity Score Weighting","url":"/docs/docs/statistical_methods/ipw/#how-are-the-regularization-parameter-and-trimming-ratio-parameter-chosen","content":"There are two options to choose the regularization parameter and trimming ratio parameter in balance: Bounding the design effect by setting max_de = X. In this case the regularization parameter and the trimming ratio parameter are chosen by a grid search over the 10 models with the largest design effect. This is based on the assumption that a larger design effect often implies better covariate balancing. Within these 10 models, the model with the smallest ASMD is chosen. Choosing the regularization parameter by the &quot;1se rule&quot; (or &quot;One Standard Error Rule&quot;) of cross validation, i.e. the largest penalty factor λ\\lambdaλ at which the MSE is at most 1 standard error from the minimal MSE . This is applied when max_de is set to None. In this case the trimming ratio parameter is set by the user, and default to 20. ","version":"Next","tagName":"h3"},{"title":"Weights estimation​","type":1,"pageTitle":"Inverse Propensity Score Weighting","url":"/docs/docs/statistical_methods/ipw/#weights-estimation","content":"The estimated propensity scores are then used to estimate the weights of the sample by setting wi=1−pipidiw_i = \\frac{1-p_i}{p_i} d_iwi​=pi​1−pi​​di​. ","version":"Next","tagName":"h3"},{"title":"References​","type":1,"pageTitle":"Inverse Propensity Score Weighting","url":"/docs/docs/statistical_methods/ipw/#references","content":"[1] Lee, S., &amp; Valliant, R. (2009). Estimation for volunteer panel web surveys using propensity score adjustment and calibration adjustment. Sociological Methods &amp; Research, 37(3), 319-343. More about Inverse Probability Weighting in Wikipedia. ","version":"Next","tagName":"h2"},{"title":"Post-Stratification","type":0,"sectionRef":"#","url":"/docs/docs/statistical_methods/poststratify/","content":"","keywords":"Post-Stratification poststratify","version":"Next"},{"title":"Introduction​","type":1,"pageTitle":"Post-Stratification","url":"/docs/docs/statistical_methods/poststratify/#introduction","content":"Post-stratification is one of the most common weighing approaches in survey statistics. It origins from a stratified sample, where the population is divided into subpopulations (strata) and the sample is conducted independently on each of them. However, when one doesn't know in advance the subpopulations to sample from (for example, when the stratum of the units in the sample is unknown in advance), or when non-response is presented, stratification can be done after the sample has been selected. The goal of post-stratification is to have the sample match exactly the joint-distribution of the target population. However, this is also the main limitation of this method. It is limited by the number of variables we are able to use for adjustment due to the nature of fitting the target exactly, and thus require a minimal number of respondent in each strata. Hence, usually at most 2 to 4 variables are used (with limited number of buckets). In addition, continues variables cannot be used for adjustment (unless bucketed). A more general approach is the inverse propensity score weighting (ipw). ","version":"Next","tagName":"h2"},{"title":"Methodology​","type":1,"pageTitle":"Post-Stratification","url":"/docs/docs/statistical_methods/poststratify/#methodology","content":"The idea behind post-stratification is simple. For each cell (strata) in the population, compute the percent of the total population in this cell. Then fit weights so that they adjust the sample so to have the same proportions for each strata as in the population. We will illustrate this with an example. Assume that we have sampled people from a certain population to a survey and asked for their age and gender so to use these for weighing. Assume also that the joint distribution of age and gender in this population is known from a census, and is the following: \tYoung adults\tAdults\tTotalFemale\t120\t380\t500 Male\t80\t420\t500 Total\t200\t800\t1000 In addition, assume that for the specific survey we ran young adults tend to reply more, so that the distribution of responses in the survey is the following: \tYoung adults\tAdults\tTotalFemale\t30\t10\t40 Male\t50\t10\t60 Total\t80\t20\t100 The post-stratification weights are then computed as follows: Proportion of Female young adults in the population is 120/1000=0.12120/1000 = 0.12120/1000=0.12Proportion of Female young adults in the sample is 30/100=0.330/100 = 0.330/100=0.3 Inflation factor - this is the inverse probability factor indicating by how much we need to multiply the total sample size to get to the total population size. It is equal to population size / sample size. In our case it is: 1000/100=101000/100 = 101000/100=10. Calculate weights for each Female young adult in the sample: (population %) / (sample %) (inflation factor). In our example this is: $0.12/0.3 10= 0.4 * 10= 4$. This means that the assigned weight of each Female young adult in the sample is 4. Similarly, we can compute the weight for people from each cell in the table: \tYoung adults\tAdultsFemale\t0.12/0.3∗10=40.12/0.3 * 10 = 40.12/0.3∗10=4\t0.38/0.1∗10=380.38/0.1 * 10 = 380.38/0.1∗10=38 Male\t0.08/0.5∗10=1.60.08/0.5 * 10 = 1.60.08/0.5∗10=1.6\t0.42/0.1∗10=420.42/0.1 *10 = 420.42/0.1∗10=42 ","version":"Next","tagName":"h2"},{"title":"Examples​","type":1,"pageTitle":"Post-Stratification","url":"/docs/docs/statistical_methods/poststratify/#examples","content":"Below are two short code examples that show how to run balance.weighting_methods.poststratify on the simulated data shipped with the package. They rely on balance.load_data so you can copy-paste the cells into a notebook, or refer to the newpost-stratification tutorial. Tip: For clarity we drop rows where any of the adjustment variables are missing, because the default strict_matching=True requires that every combination observed in the sample also appears in the target data. ","version":"Next","tagName":"h2"},{"title":"Matching a single variable​","type":1,"pageTitle":"Post-Stratification","url":"/docs/docs/statistical_methods/poststratify/#matching-a-single-variable","content":"import pandas as pd from balance import load_data from balance.weighting_methods.poststratify import poststratify target_df, sample_df = load_data() sample_gender = sample_df.dropna(subset=[&quot;gender&quot;]) target_gender = target_df.dropna(subset=[&quot;gender&quot;]) result = poststratify( sample_df=sample_gender[[&quot;gender&quot;]], sample_weights=pd.Series(1, index=sample_gender.index), target_df=target_gender[[&quot;gender&quot;]], target_weights=pd.Series(1, index=target_gender.index), ) weighted = sample_gender.assign(weight=result[&quot;weight&quot;]) display(weighted.groupby(&quot;gender&quot;)[&quot;weight&quot;].sum())  The grouped sum reproduces the target population counts (because we pass unit design weights): each gender sums to 4551 in this dataset. You can divide by the total weight to recover the target proportions. ","version":"Next","tagName":"h3"},{"title":"Matching the joint distribution of two variables​","type":1,"pageTitle":"Post-Stratification","url":"/docs/docs/statistical_methods/poststratify/#matching-the-joint-distribution-of-two-variables","content":"covariates = [&quot;gender&quot;, &quot;age_group&quot;] sample_cells = sample_df.dropna(subset=covariates) target_cells = target_df.dropna(subset=covariates) result = poststratify( sample_df=sample_cells[covariates], sample_weights=pd.Series(1, index=sample_cells.index), target_df=target_cells[covariates], target_weights=pd.Series(1, index=target_cells.index), ) weighted = sample_cells.assign(weight=result[&quot;weight&quot;]) display(weighted.groupby(covariates)[&quot;weight&quot;].sum().unstack())  This second example uses the same two categorical variables but keeps their joint cells intact. The pivoted table matches the census counts for each cell (e.g. Female aged 25-34 totals 1360 and Male aged 18-24totals 905). Unlike raking, which iteratively matches the marginal distributions of each variable, poststratify calculates weights per cell so that the final weighted sample matches the full two-dimensional distribution. ","version":"Next","tagName":"h3"},{"title":"References​","type":1,"pageTitle":"Post-Stratification","url":"/docs/docs/statistical_methods/poststratify/#references","content":"More about post-stratification: Introduction to post-stratificationKolenikov, Stas. 2016. “Post-Stratification or Non-Response Adjustment?” Survey Practice 9 (3). https://doi.org/10.29115/SP-2016-0014. ","version":"Next","tagName":"h2"},{"title":"Covariate Balancing Propensity Score (CBPS)","type":0,"sectionRef":"#","url":"/docs/docs/statistical_methods/cbps/","content":"","keywords":"Covariate Balancing Propensity Score cbps","version":"Next"},{"title":"References and implementation​","type":1,"pageTitle":"Covariate Balancing Propensity Score (CBPS)","url":"/docs/docs/statistical_methods/cbps/#references-and-implementation","content":"Reference: Imai, K., &amp; Ratkovic, M. (2014). Covariate balancing propensity score. Journal of the Royal Statistical Society: Series B: Statistical Methodology, 243-263. (link) R package: https://cran.r-project.org/web/packages/CBPS/ (github repo ) The implementation of CBPS in balance is based on the R package, but is enhanced so to match balance's workflow by adding: features transformations, ability to bound the design effect by running a constrained optimization, and weight trimming. For the implementation in balance see code. The CBPS implementation in balance was written by Luke Sonnet and Tal Sarig. ","version":"Next","tagName":"h2"},{"title":"Introduction​","type":1,"pageTitle":"Covariate Balancing Propensity Score (CBPS)","url":"/docs/docs/statistical_methods/cbps/#introduction","content":"Goal: Estimate the propensity score that will also result in maximizing the covariate balance. Background: When estimating propensity score, there is often a process of adjusting the model and choosing the covariates for better covariate balancing. The goal of CBPS is to allow the researcher to avoid this iterative process and suggest an estimator that is optimizing both the propensity score and the balance of the covariates together. Advantages of this method over propensity score methods: Preferable in the cases of misspecification of the propensity score model, which may lead to a bias in the estimated measure.Simple to adjust and extend to other settings in causal inference.Inherits theoretical properties of GMM (generalized method of moments) and EL (empirical likelihood), which offers some theoretical guarantees of the method. ","version":"Next","tagName":"h2"},{"title":"Methodology​","type":1,"pageTitle":"Covariate Balancing Propensity Score (CBPS)","url":"/docs/docs/statistical_methods/cbps/#methodology","content":"A full description of the methodology and details are described in Imai and Ratkovic (2014). We provide here a short description of the methodology. Consider a sample of respondents of size nnn and a random sample from a target populaiton of size NNN. For each i∈Sample∪Targeti \\in Sample \\cup Targeti∈Sample∪Target, let IiI_iIi​ be the indicator for inclusion in sample (0 for target and 1 for sample) and XiX_iXi​ be a vector of observed covariates. The propensity score is defined as the conditional probability of being included in the sample conditioned on the covariates, P(Ii=1∣Xi=x)P(I_i=1 | X_i=x)P(Ii​=1∣Xi​=x). Let YiY_iYi​ be the potential outcome observed only for i∈Samplei\\in Samplei∈Sample. ","version":"Next","tagName":"h2"},{"title":"Assumptions​","type":1,"pageTitle":"Covariate Balancing Propensity Score (CBPS)","url":"/docs/docs/statistical_methods/cbps/#assumptions","content":"The propensity is bounded away from 0 and 1 (all individuals have a theoretical probability to be in the respondents group): 0&lt;P(Ii=1∣Xi=x)&lt;10&lt;P(I_i=1 | X_i=x)&lt;10&lt;P(Ii​=1∣Xi​=x)&lt;1 for all xxx.Ignorability assumption: ((Yi(0),Yi(1))⊥Ii)∣Xi({(Y_i(0), Y_i(1))}\\perp I_i) | X_i((Yi​(0),Yi​(1))⊥Ii​)∣Xi​, where Yi(0)Y_i(0)Yi​(0) indicates the response of unit iii if it is from the sample, and Yi(1)Y_i(1)Yi​(1) indicates the hypothetical response of unit iii if it is from the target population. Rosenbaum and Rubin (1983) [2] showed that this assumption implies that the outcome is independent of the inclusion in the sample given the (theoretical) propensity score (this is the &quot;dimension reduction&quot; property of the propensity score). I.e.: ((Yi(0),Yi(1))⊥Ii)∣P(Ii=1∣Xi=x)({(Y_i(0), Y_i(1))}\\perp I_i) | P(I_i=1 | X_i=x)((Yi​(0),Yi​(1))⊥Ii​)∣P(Ii​=1∣Xi​=x). ","version":"Next","tagName":"h3"},{"title":"Recap - Propensity score estimation​","type":1,"pageTitle":"Covariate Balancing Propensity Score (CBPS)","url":"/docs/docs/statistical_methods/cbps/#recap---propensity-score-estimation","content":"Using a logistic regression model, the propensity score is modeled by: πβ(Xi)=P(Ii=1∣Xi=x)=exp⁡(XiTβ)1+exp⁡(XiTβ)\\pi _\\beta(X_i)=P(I_i=1|X_i=x)=\\frac{\\exp(X_i ^T \\beta)}{1+\\exp(X_i ^T \\beta)}πβ​(Xi​)=P(Ii​=1∣Xi​=x)=1+exp(XiT​β)exp(XiT​β)​ for all i∈Samplei \\in Samplei∈Sample. This is estimated by maximizing the log-likelihood, which results in: β^MLE=arg⁡max⁡β∑i=1nIilog⁡(πβ(Xi))+(1−Ii)log⁡(1−πβ(Xi))\\hat{\\beta}_{MLE}=\\arg\\max_\\beta \\sum_{i=1}^n I_i\\log(\\pi_\\beta(X_i))+(1-I_i)\\log(1-\\pi_\\beta(X_i))β^​MLE​=argβmax​i=1∑n​Ii​log(πβ​(Xi​))+(1−Ii​)log(1−πβ​(Xi​)) which implies the first order condition: 1n∑i=1n[Iiπβ′(Xi)πβ(Xi)+(1−Ii)πβ′(Xi)1−πβ(Xi)]=0\\frac{1}{n}\\sum_{i=1}^n \\left[ \\frac{I_i\\pi^\\prime_\\beta(X_i)}{\\pi_\\beta(X_i)} +\\frac{(1-I_i)\\pi^\\prime_\\beta(X_i)}{1-\\pi_\\beta(X_i)}\\right]=0n1​i=1∑n​[πβ​(Xi​)Ii​πβ′​(Xi​)​+1−πβ​(Xi​)(1−Ii​)πβ′​(Xi​)​]=0 where the derivative of π\\piπ is by βT\\beta^TβT. This condition can be viewed as a condition that balances a certain function of the covariates, in this case the derivative of the propensity score πβ′(Xi)\\pi^\\prime_\\beta(X_i)πβ′​(Xi​). ","version":"Next","tagName":"h3"},{"title":"CBPS​","type":1,"pageTitle":"Covariate Balancing Propensity Score (CBPS)","url":"/docs/docs/statistical_methods/cbps/#cbps","content":"Generally, we can expand the above to hold for any function fff: E{Iif(Xi)πβ(Xi)+(1−Ii)f(Xi)1−πβ(Xi)}=0\\mathbb{E} \\left\\{ \\frac{I_if(X_i)}{\\pi_\\beta(X_i)} +\\frac{(1-I_i)f(X_i)}{1-\\pi_\\beta(X_i)}\\right\\} =0E{πβ​(Xi​)Ii​f(Xi​)​+1−πβ​(Xi​)(1−Ii​)f(Xi​)​}=0 (given the expectation exists). CBPS chooses f(x)=xf(x)=xf(x)=x as the balancing function fff in addition to the traditional logistic regression condition (this is what implemented in R and in balance), but generally any function the researcher may choose could be used here. The function f(x)=xf(x)=xf(x)=x results in balancing the first moment of each covariate ","version":"Next","tagName":"h3"},{"title":"Estimation of CBPS​","type":1,"pageTitle":"Covariate Balancing Propensity Score (CBPS)","url":"/docs/docs/statistical_methods/cbps/#estimation-of-cbps","content":"The estimation is done by using Generalized Methods of Moments (GMM):Given moments conditions of the form E{g(Xi,θ)}=0\\mathbb{E}\\{g(X_i,\\theta)\\}=0E{g(Xi​,θ)}=0, the optimal solution minimizes the norm of the sample analog, 1n∑i=1ng(xi,θ)\\frac{1}{n}\\sum_{i=1}^n g(x_i,\\theta)n1​∑i=1n​g(xi​,θ), with respect to θ\\thetaθ. This results in an estimator of the form: θ^=arg⁡min⁡θ1n∑i=1ngT(xi,θ)Wg(xi,θ),\\hat{\\theta}=\\arg\\min_\\theta \\frac{1}{n}\\sum_{i=1}^n g^T(x_i,\\theta)Wg(x_i,\\theta),θ^=argθmin​n1​i=1∑n​gT(xi​,θ)Wg(xi​,θ), where WWW is semi-definite positive matrix, often chosen to be the variance matrix W(θ)=(1n∑i=1ng(xi,θ)gT(xi,θ))−1W(\\theta)=\\left(\\frac{1}{n}\\sum_{i=1}^n g(x_i,\\theta)g^T(x_i,\\theta)\\right)^{-1}W(θ)=(n1​∑i=1n​g(xi​,θ)gT(xi​,θ))−1 (which is unknown). This can be solved by iterative algorithm, by starting with W^=I\\hat{W}=IW^=I, computing θ^\\hat{\\theta}θ^ and W(θ^)W(\\hat{\\theta})W(θ^), and so on (for two-step GMM, we stop after optimizing to θ^\\hat{\\theta}θ^). Alternatively, it can be solved by Continuously updating GMM algorithm, which estimate θ^\\hat{\\theta}θ^ and W^\\hat{W}W^ on the same time. The model is over-identified if the number of equations is larger than the number of parameters. For CBPS, the sample analog for the covariate balancing moment condition is:1n∑i=1n[Iixiπβ(xi)+(1−Ii)xi1−πβ(xi)]\\frac{1}{n}\\sum_{i=1}^n\\left[\\frac{I_ix_i}{\\pi_\\beta(x_i)} +\\frac{(1-I_i)x_i}{1-\\pi_\\beta(x_i)}\\right]n1​∑i=1n​[πβ​(xi​)Ii​xi​​+1−πβ​(xi​)(1−Ii​)xi​​], which can be written as 1n∑i=1nIi−πβ(Xi)πβ(xi)(1−πβ(xi))xi=0\\frac{1}{n}\\sum_{i=1}^n\\frac{I_i-\\pi_\\beta(X_i)}{\\pi_\\beta(x_i)(1-\\pi_\\beta(x_i))}x_i=0n1​∑i=1n​πβ​(xi​)(1−πβ​(xi​))Ii​−πβ​(Xi​)​xi​=0 (for Ii∈{0,1}I_i\\in\\{0,1\\}Ii​∈{0,1}). Let gi(Ii,Xi)=(Ii−πβ(Xi)πβ(Xi)(1−πβ(Xi))πβ′(Xi) Ii−πβ(Xi)πβ(Xi)(1−πβ(Xi))Xi )g_i(I_i,X_i)=\\left(\\begin{matrix} \\frac{I_i-\\pi_\\beta(X_i)}{\\pi_\\beta(X_i)(1-\\pi_\\beta(X_i))}\\pi^\\prime_\\beta(X_i)\\ \\\\ \\frac{I_i-\\pi_\\beta(X_i)}{\\pi_\\beta(X_i)(1-\\pi_\\beta(X_i))} X_i\\ \\end{matrix}\\right)gi​(Ii​,Xi​)=(πβ​(Xi​)(1−πβ​(Xi​))Ii​−πβ​(Xi​)​πβ′​(Xi​) πβ​(Xi​)(1−πβ​(Xi​))Ii​−πβ​(Xi​)​Xi​ ​) be the vector representing the moments we would like to solve. This contains the two conditions of maximizing the log-likelihood and balancing the covariates. Note that this is over-identified, since the number of equations is larger then the number of parameters. Another option is to consider the “just-identified” (&quot;exact&quot;) CBPS, where we consider only the covariate balancing conditions and not the propensity score condition. Using GMM, we have β^=arg⁡min⁡βgˉTΣβ−1gˉ\\hat{\\beta}=\\arg\\min_\\beta \\bar{g}^T \\Sigma^{-1}_\\beta \\bar{g}β^​=argβmin​gˉ​TΣβ−1​gˉ​ where gˉ=1n∑i=1ngi\\bar{g}=\\frac{1}{n}\\sum_{i=1}^n g_igˉ​=n1​∑i=1n​gi​ and Σβ=E[1n∑i=1ngigiT∣Xi]\\Sigma_\\beta=\\mathbb{E}\\left[\\frac{1}{n}\\sum_{i=1}^n g_i g^T_i | X_i\\right]Σβ​=E[n1​∑i=1n​gi​giT​∣Xi​], which can be estimated by Σ^β=1n∑i=1n(πβ(Xi)(1−πβ(Xi))XiXiTXiXiTXiXiTπβ(Xi)(1−πβ(Xi))XiXiT)\\hat{\\Sigma}_\\beta=\\frac{1}{n}\\sum_{i=1}^n \\left( \\begin{matrix} \\pi_\\beta(X_i)(1-\\pi_\\beta(X_i))X_iX_i^T &amp; X_i X_i^T \\\\ X_iX_i^T &amp; \\pi_\\beta(X_i)(1-\\pi_\\beta(X_i))X_iX_i^T \\end{matrix} \\right)Σ^β​=n1​i=1∑n​(πβ​(Xi​)(1−πβ​(Xi​))Xi​XiT​Xi​XiT​​Xi​XiT​πβ​(Xi​)(1−πβ​(Xi​))Xi​XiT​​) To optimize this, we use the two-step GMM, using gradient-based optimization, starting with βMLE\\beta^{MLE}βMLE (from the original logistic regression): β0=β^MLE\\beta_0=\\hat{\\beta}_{MLE}β0​=β^​MLE​W^0=Σβ0−1\\hat{W}_0=\\Sigma_{\\beta_0}^{-1}W^0​=Σβ0​−1​β^=arg⁡min⁡βgˉTW^β^0gˉ\\hat{\\beta}=\\arg\\min_\\beta \\bar{g}^T\\hat{W}_{\\hat{\\beta}_0}\\bar{g}β^​=argminβ​gˉ​TW^β^​0​​gˉ​ - use gradient based optimization ","version":"Next","tagName":"h3"},{"title":"References​","type":1,"pageTitle":"Covariate Balancing Propensity Score (CBPS)","url":"/docs/docs/statistical_methods/cbps/#references","content":"[1] Imai, K., &amp; Ratkovic, M. (2014). Covariate balancing propensity score. Journal of the Royal Statistical Society: Series B: Statistical Methodology, 243-263. [2] PAUL R. ROSENBAUM, DONALD B. RUBIN, The central role of the propensity score in observational studies for causal effects, Biometrika, Volume 70, Issue 1, April 1983, Pages 41–55, https://doi.org/10.1093/biomet/70.1.41 ","version":"Next","tagName":"h2"}]